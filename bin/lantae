#!/usr/bin/env ruby

require 'json'
require 'net/http'
require 'uri'
require 'optparse'
require 'readline'
require 'tempfile'
require 'logger'
require 'fileutils'
require 'aws-sdk-secretsmanager'
require 'aws-sdk-bedrockruntime'
require_relative '../lib/ruby/mcp_manager'
require_relative '../lib/ruby/planning_agent'
require_relative '../lib/ruby/task_analyzer'
require_relative '../lib/ruby/execution_engine'
require_relative '../lib/ruby/task_database'
require_relative '../lib/ruby/auto_fixer'
require_relative '../lib/ruby/lsp/client'
require_relative '../lib/ruby/lsp/server_manager'
require_relative '../lib/ruby/lsp/auto_starter'
require_relative '../lib/ruby/provider_detector'
require_relative '../lib/ruby/conversation_manager'
require_relative '../lib/ruby/prompt_templates'
require_relative '../lib/ruby/cost_tracker'
require_relative '../lib/ruby/streaming_handler'
require_relative '../lib/ruby/interactive_enhancements'
require_relative '../lib/ruby/plugin_system'
require_relative '../lib/ruby/enhanced_repl'
require_relative '../lib/ruby/response_formatter'
require_relative '../lib/ruby/api_key_authorizer'
require_relative '../lib/ruby/intelligent_router'
require_relative '../lib/ruby/planning_agent'

VERSION = '1.0.0'

class SecretManager
  def initialize(region = 'us-east-1', secret_name = 'lantae/api-keys')
    @region = region
    @secret_name = secret_name
    @client = nil
    @cached_secrets = {}
  end

  def get_api_key(provider)
    # First check environment variable
    env_key = "#{provider.upcase}_API_KEY"
    return ENV[env_key] if ENV[env_key]

    # Check cached secrets
    if @cached_secrets[provider]
      ENV[env_key] = @cached_secrets[provider]
      return @cached_secrets[provider]
    end

    # Fetch from AWS Secrets Manager
    begin
      init_client
      response = @client.get_secret_value(secret_id: @secret_name)
      secrets = JSON.parse(response.secret_string)
      
      # Cache all secrets and set environment variables
      secrets.each do |key, value|
        @cached_secrets[key] = value
        ENV["#{key.upcase}_API_KEY"] = value
      end
      
      @cached_secrets[provider]
    rescue Aws::SecretsManager::Errors::ResourceNotFoundException
      raise "AWS Secret '#{@secret_name}' not found. Create it with your API keys."
    rescue => e
      raise "Failed to retrieve API keys from AWS Secrets Manager: #{e.message}"
    end
  end

  private

  def init_client
    return if @client
    @client = Aws::SecretsManager::Client.new(region: @region)
  end
end

# Shared progress indicator class for all providers
class ProgressIndicator
  def initialize(message = "Processing")
    @message = message
    @running = false
    @thread = nil
    @start_time = nil
  end
  
  def start
    @running = true
    @start_time = Time.now
    @thread = Thread.new do
      spinner_chars = ['⠋', '⠙', '⠹', '⠸', '⠼', '⠴', '⠦', '⠧', '⠇', '⠏']
      i = 0
      while @running
        elapsed = Time.now - @start_time
        elapsed_str = elapsed > 2 ? " (#{elapsed.round(1)}s)" : ""
        print "\r🤖 #{spinner_chars[i % spinner_chars.length]} #{@message}#{elapsed_str}..."
        $stdout.flush
        sleep 0.1
        i += 1
      end
    end
  end
  
  def update_message(new_message)
    @message = new_message
  end
  
  def stop
    @running = false
    @thread&.join(0.5)  # Wait max 0.5 seconds
    @thread&.kill if @thread&.alive?
    print "\r" + " " * 100 + "\r"  # Clear the line
    $stdout.flush
  end
end

class ProviderManager
  def initialize(secret_manager, tool_manager = nil)
    @secret_manager = secret_manager
    @tool_manager = tool_manager
    @providers = {
      'ollama' => OllamaProvider.new,
      'openai' => OpenAIProvider.new(secret_manager),
      'anthropic' => AnthropicProvider.new(secret_manager),
      'bedrock' => BedrockProvider.new,
      'gemini' => GeminiProvider.new(secret_manager),
      'mistral' => MistralProvider.new(secret_manager),
      'perplexity' => PerplexityProvider.new(secret_manager)
    }
    @current_provider = 'anthropic'  # Default to Anthropic
    @current_model = 'claude-3-5-sonnet-20241022'  # Default Claude model
    
    # Set tool manager for all providers
    if @tool_manager
      @providers.each do |name, provider|
        provider.set_tool_manager(@tool_manager) if provider.respond_to?(:set_tool_manager)
      end
    end
  end

  attr_accessor :current_model, :current_provider

  def switch_provider(provider, model = nil)
    raise "Provider '#{provider}' not supported. Available: #{@providers.keys.join(', ')}" unless @providers[provider]
    
    @current_provider = provider
    if model
      @current_model = model
    else
      # Set default model for provider
      defaults = {
        'ollama' => 'cogito:latest',
        'openai' => 'gpt-4o',
        'anthropic' => 'claude-3-5-sonnet-20241022',
        'claude' => 'claude-3-5-sonnet-20241022',
        'bedrock' => 'claude-3-sonnet',
        'gemini' => 'gemini-1.5-pro',
        'mistral' => 'mistral-large-latest',
        'perplexity' => 'llama-3.1-sonar-large-128k-online'
      }
      @current_model = defaults[provider]
    end
  end

  def chat(messages, options = {})
    provider = @providers[@current_provider]
    
    # Show dispatch status unless in quiet mode
    unless options[:quiet]
      puts "\n🔄 Dispatching to #{@current_provider} provider (#{@current_model})..."
      $stdout.flush
    end
    
    provider.chat(@current_model, messages, options)
  end

  def list_models
    provider = @providers[@current_provider]
    provider.list_models
  end

  def get_provider_info
    { provider: @current_provider, model: @current_model }
  end
end

class OllamaProvider
  def initialize(base_url = 'http://localhost:11434')
    @base_url = base_url
    @tool_manager = nil
  end

  def set_tool_manager(tool_manager)
    @tool_manager = tool_manager
  end

  def chat(model, messages, options = {})
    uri = URI("#{@base_url}/api/chat")
    http = Net::HTTP.new(uri.host, uri.port)
    http.read_timeout = 300
    http.open_timeout = 30
    
    # Add tool context to the system message
    enhanced_messages = messages.dup
    if @tool_manager && !enhanced_messages.empty?
      tools_context = @tool_manager.get_tools_context
      system_message = {
        role: 'system',
        content: "You are an AI assistant with access to various tools for file operations and system commands. #{tools_context}\n\nWhen you want to use a tool, include a TOOL_CALL in your response. You can make multiple tool calls in a single response. After each tool call, I will provide the result, and you can continue your response or make additional tool calls as needed.\n\nAlways explain what you're doing and why before using tools. Be helpful and thorough in your responses."
      }
      
      # Insert system message at the beginning if it doesn't exist, or merge with existing system message
      if enhanced_messages[0] && enhanced_messages[0][:role] == 'system'
        enhanced_messages[0][:content] = system_message[:content] + "\n\n" + enhanced_messages[0][:content]
      else
        enhanced_messages.unshift(system_message)
      end
    end
    
    request = Net::HTTP::Post.new(uri)
    request['Content-Type'] = 'application/json'
    request.body = {
      model: model,
      messages: enhanced_messages,
      stream: false,
      options: {
        temperature: (options[:temperature] || 0.1).to_f
      }
    }.to_json

    begin
      # Show progress indicator
      progress = ProgressIndicator.new("Preparing Ollama request")
      progress.start unless options[:no_spinner]
      
      # Update status
      progress&.update_message("Sending request to local Ollama (#{model})")
      
      response = http.request(request)
      
      # Update status
      progress&.update_message("Processing Ollama response")
      
      # Stop progress indicator
      progress&.stop
      
      data = JSON.parse(response.body)
      
      # Handle error responses from Ollama
      if data['error']
        raise "Ollama error: #{data['error']}"
      end
      
      # Safely access the content
      content = data.dig('message', 'content')
      unless content
        raise "Unexpected response format from Ollama. Response: #{data.inspect}"
      end
      
      # Process tool calls in the response
      content = process_tool_calls(content) if @tool_manager
      
      content
    rescue Errno::ECONNREFUSED
      if spinner_thread
        spinner_thread.kill
        print "\r" + " " * 20 + "\r"
      end
      raise 'Cannot connect to Ollama server. Make sure Ollama is running.'
    end
  end

  def process_tool_calls(content)
    processed_content = content.dup
    
    content.scan(/TOOL_CALL:\s*([^\n]+)/) do |tool_call_match|
      tool_call = tool_call_match[0].strip
      tool_name, *args = tool_call.split(' ')
      
      begin
        puts "\n🔧 Tool requested: #{tool_name}"
        puts "   📥 Processing tool call in response..."
        $stdout.flush
        
        result = @tool_manager.execute_tool(tool_name, args.join(' '))
        
        # Replace the tool call with the result
        tool_call_line = "TOOL_CALL: #{tool_call}"
        replacement = "#{tool_call_line}\n\nTool Result:\n```\n#{result}\n```\n"
        processed_content = processed_content.gsub(tool_call_line, replacement)
        
        puts "   ✅ Tool completed: #{tool_name}"
        # Format tool output for better readability
        formatted_output = Lantae::ResponseFormatter.format_tool_output(tool_name, result, true)
        puts formatted_output
      rescue => error
        error_msg = "Error executing #{tool_name}: #{error.message}"
        puts "   ❌ Tool failed: #{tool_name}"
        puts "   🚫 Error: #{error.message}"
        
        tool_call_line = "TOOL_CALL: #{tool_call}"
        replacement = "#{tool_call_line}\n\nTool Error:\n```\n#{error_msg}\n```\n"
        processed_content = processed_content.gsub(tool_call_line, replacement)
      end
    end
    
    processed_content
  end

  def list_models
    uri = URI("#{@base_url}/api/tags")
    http = Net::HTTP.new(uri.host, uri.port)
    http.read_timeout = 30
    http.open_timeout = 10
    
    begin
      response = http.get(uri)
      data = JSON.parse(response.body)
      (data['models'] || []).map { |m| m['name'] }
    rescue Errno::ECONNREFUSED
      raise 'Cannot connect to Ollama server. Make sure Ollama is running.'
    end
  end

  private
  
  # Removed start_spinner method as we now use shared ProgressIndicator
end

class OpenAIProvider
  def initialize(secret_manager)
    @secret_manager = secret_manager
    @tool_manager = nil
  end

  def set_tool_manager(tool_manager)
    @tool_manager = tool_manager
  end

  def chat(model, messages, options = {})
    api_key = @secret_manager.get_api_key('openai')
    raise 'OpenAI API key not found in environment or AWS Secrets Manager' unless api_key

    # Add tool context to the system message
    enhanced_messages = enhance_messages_with_tools(messages)

    uri = URI('https://api.openai.com/v1/chat/completions')
    http = Net::HTTP.new(uri.host, uri.port)
    http.use_ssl = true

    request = Net::HTTP::Post.new(uri)
    request['Authorization'] = "Bearer #{api_key}"
    request['Content-Type'] = 'application/json'
    request.body = {
      model: model,
      messages: enhanced_messages,
      temperature: (options[:temperature] || 0.1).to_f,
      max_tokens: 4096
    }.to_json
    
    # Show progress indicator
    progress = ProgressIndicator.new("Preparing OpenAI request")
    progress.start unless options[:no_spinner]

    begin
      # Update status
      progress&.update_message("Sending request to OpenAI API")
      
      response = http.request(request)
      
      # Update status
      progress&.update_message("Processing OpenAI response")
      
      # Stop progress indicator
      progress&.stop
      
      if response.code == '401'
        raise 'Invalid OpenAI API key. Check your credentials.'
      end

      data = JSON.parse(response.body)
      
      # Handle error responses
      if data['error']
        raise "OpenAI error: #{data['error']['message'] || data['error']}"
      end
      
      # Safely access the content
      content = data.dig('choices', 0, 'message', 'content')
      unless content
        raise "Unexpected response format from OpenAI. Response: #{data.inspect}"
      end
      
      # Process tool calls in the response
      content = process_tool_calls(content) if @tool_manager
      
      content
    ensure
      # Make sure progress indicator is stopped
      progress&.stop
    end
  end

  private

  def enhance_messages_with_tools(messages)
    enhanced_messages = messages.dup
    
    if @tool_manager && !enhanced_messages.empty?
      tools_context = @tool_manager.get_tools_context
      system_message = {
        role: 'system',
        content: "You are an AI assistant with access to various tools for file operations and system commands. #{tools_context}\n\nWhen you want to use a tool, include a TOOL_CALL in your response. You can make multiple tool calls in a single response. After each tool call, I will provide the result, and you can continue your response or make additional tool calls as needed.\n\nAlways explain what you're doing and why before using tools. Be helpful and thorough in your responses."
      }
      
      # Insert system message at the beginning if it doesn't exist, or merge with existing system message
      if enhanced_messages[0] && enhanced_messages[0][:role] == 'system'
        enhanced_messages[0][:content] = system_message[:content] + "\n\n" + enhanced_messages[0][:content]
      else
        enhanced_messages.unshift(system_message)
      end
    end
    
    enhanced_messages
  end

  def process_tool_calls(content)
    processed_content = content.dup
    
    content.scan(/TOOL_CALL:\s*([^\n]+)/) do |tool_call_match|
      tool_call = tool_call_match[0].strip
      tool_name, *args = tool_call.split(' ')
      
      begin
        puts "\n🔧 Tool requested: #{tool_name}"
        puts "   📥 Processing tool call in response..."
        $stdout.flush
        
        result = @tool_manager.execute_tool(tool_name, args.join(' '))
        
        # Replace the tool call with the result
        tool_call_line = "TOOL_CALL: #{tool_call}"
        replacement = "#{tool_call_line}\n\nTool Result:\n```\n#{result}\n```\n"
        processed_content = processed_content.gsub(tool_call_line, replacement)
        
        puts "   ✅ Tool completed: #{tool_name}"
        # Format tool output for better readability
        formatted_output = Lantae::ResponseFormatter.format_tool_output(tool_name, result, true)
        puts formatted_output
      rescue => error
        error_msg = "Error executing #{tool_name}: #{error.message}"
        puts "   ❌ Tool failed: #{tool_name}"
        puts "   🚫 Error: #{error.message}"
        
        tool_call_line = "TOOL_CALL: #{tool_call}"
        replacement = "#{tool_call_line}\n\nTool Error:\n```\n#{error_msg}\n```\n"
        processed_content = processed_content.gsub(tool_call_line, replacement)
      end
    end
    
    processed_content
  end

  def list_models
    %w[o1-preview o1-mini]
  end
end

class AnthropicProvider
  def initialize(secret_manager)
    @secret_manager = secret_manager
    @tool_manager = nil
  end

  def set_tool_manager(tool_manager)
    @tool_manager = tool_manager
  end

  def chat(model, messages, options = {})
    api_key = @secret_manager.get_api_key('anthropic')
    
    # If no API key found, request one via browser flow
    unless api_key
      puts "\n🔍 No Anthropic API key found..."
      api_key = Lantae::APIKeyAuthorizer.request_anthropic_key
      unless api_key
        raise 'Anthropic API key is required. Please run lantae again to set up your key.'
      end
    end

    # Prepare messages and extract system content
    system_content = nil
    enhanced_messages = messages.dup
    
    # Extract system messages and tool context
    if @tool_manager
      tools_context = @tool_manager.get_tools_context
      system_content = "You are an AI assistant with access to various tools for file operations and system commands. #{tools_context}\n\nWhen you want to use a tool, include a TOOL_CALL in your response. You can make multiple tool calls in a single response. After each tool call, I will provide the result, and you can continue your response or make additional tool calls as needed.\n\nAlways explain what you're doing and why before using tools. Be helpful and thorough in your responses."
    end
    
    # Extract any existing system messages
    system_messages = enhanced_messages.select { |msg| msg[:role] == 'system' }
    if system_messages.any?
      existing_system = system_messages.map { |msg| msg[:content] }.join("\n\n")
      system_content = system_content ? "#{system_content}\n\n#{existing_system}" : existing_system
    end
    
    # Remove system messages from the array
    enhanced_messages.reject! { |msg| msg[:role] == 'system' }

    uri = URI('https://api.anthropic.com/v1/messages')
    http = Net::HTTP.new(uri.host, uri.port)
    http.use_ssl = true

    request = Net::HTTP::Post.new(uri)
    request['x-api-key'] = api_key
    request['anthropic-version'] = '2023-06-01'
    request['Content-Type'] = 'application/json'
    
    request_body = {
      model: model,
      max_tokens: 4096,
      temperature: (options[:temperature] || 0.1).to_f,
      messages: enhanced_messages
    }
    
    # Add system parameter if we have system content
    request_body[:system] = system_content if system_content
    
    request.body = request_body.to_json

    # Show progress indicator
    progress = ProgressIndicator.new("Preparing Anthropic request")
    progress.start unless options[:no_spinner]
    
    begin
      # Update status
      progress&.update_message("Sending request to Anthropic API")
      
      response = http.request(request)
      
      # Update status
      progress&.update_message("Processing Anthropic response")
      
      # Stop progress indicator
      progress&.stop
      
      if response.code == '401'
        raise 'Invalid Anthropic API key. Check your credentials.'
      end

      data = JSON.parse(response.body)
      
      # Handle error responses
      if data['error']
        raise "Anthropic error: #{data['error']['message'] || data['error']}"
      end
      
      # Safely access the content
      content = data.dig('content', 0, 'text')
      unless content
        raise "Unexpected response format from Anthropic. Response: #{data.inspect}"
      end
      
      # Process tool calls in the response
      content = process_tool_calls(content) if @tool_manager
      
      content
    ensure
      # Make sure progress indicator is stopped
      progress&.stop
    end
  end

  def list_models
    %w[claude-3-5-sonnet-20241022 claude-3-5-haiku-20241022 claude-3-opus-20240229 claude-3-sonnet-20240229 claude-3-haiku-20240307]
  end

  private

  def enhance_messages_with_tools(messages)
    enhanced_messages = messages.dup
    
    if @tool_manager && !enhanced_messages.empty?
      tools_context = @tool_manager.get_tools_context
      system_message = {
        role: 'system',
        content: "You are an AI assistant with access to various tools for file operations and system commands. #{tools_context}\n\nWhen you want to use a tool, include a TOOL_CALL in your response. You can make multiple tool calls in a single response. After each tool call, I will provide the result, and you can continue your response or make additional tool calls as needed.\n\nAlways explain what you're doing and why before using tools. Be helpful and thorough in your responses."
      }
      
      # Insert system message at the beginning if it doesn't exist, or merge with existing system message
      if enhanced_messages[0] && enhanced_messages[0][:role] == 'system'
        enhanced_messages[0][:content] = system_message[:content] + "\n\n" + enhanced_messages[0][:content]
      else
        enhanced_messages.unshift(system_message)
      end
    end
    
    enhanced_messages
  end

  def process_tool_calls(content)
    processed_content = content.dup
    
    content.scan(/TOOL_CALL:\s*([^\n]+)/) do |tool_call_match|
      tool_call = tool_call_match[0].strip
      tool_name, *args = tool_call.split(' ')
      
      begin
        puts "\n🔧 Tool requested: #{tool_name}"
        puts "   📥 Processing tool call in response..."
        $stdout.flush
        
        result = @tool_manager.execute_tool(tool_name, args.join(' '))
        
        # Replace the tool call with the result
        tool_call_line = "TOOL_CALL: #{tool_call}"
        replacement = "#{tool_call_line}\n\nTool Result:\n```\n#{result}\n```\n"
        processed_content = processed_content.gsub(tool_call_line, replacement)
        
        puts "   ✅ Tool completed: #{tool_name}"
        # Format tool output for better readability
        formatted_output = Lantae::ResponseFormatter.format_tool_output(tool_name, result, true)
        puts formatted_output
      rescue => error
        error_msg = "Error executing #{tool_name}: #{error.message}"
        puts "   ❌ Tool failed: #{tool_name}"
        puts "   🚫 Error: #{error.message}"
        
        tool_call_line = "TOOL_CALL: #{tool_call}"
        replacement = "#{tool_call_line}\n\nTool Error:\n```\n#{error_msg}\n```\n"
        processed_content = processed_content.gsub(tool_call_line, replacement)
      end
    end
    
    processed_content
  end
end


class GeminiProvider
  def initialize(secret_manager)
    @secret_manager = secret_manager
    @tool_manager = nil
  end

  def set_tool_manager(tool_manager)
    @tool_manager = tool_manager
  end

  def chat(model, messages, options = {})
    api_key = @secret_manager.get_api_key('gemini')
    raise 'Gemini API key not found in environment or AWS Secrets Manager' unless api_key

    # Add tool context to the messages
    enhanced_messages = enhance_messages_with_tools(messages)

    # Convert messages to Gemini format
    contents = enhanced_messages.map do |msg|
      {
        role: msg[:role] == 'assistant' ? 'model' : 'user',
        parts: [{ text: msg[:content] }]
      }
    end

    uri = URI("https://generativelanguage.googleapis.com/v1beta/models/#{model}:generateContent?key=#{api_key}")
    http = Net::HTTP.new(uri.host, uri.port)
    http.use_ssl = true

    request = Net::HTTP::Post.new(uri)
    request['Content-Type'] = 'application/json'
    request.body = {
      contents: contents,
      generationConfig: {
        temperature: (options[:temperature] || 0.1).to_f,
        maxOutputTokens: 4096
      }
    }.to_json

    # Show progress indicator
    progress = ProgressIndicator.new("Preparing Gemini request")
    progress.start unless options[:no_spinner]
    
    begin
      # Update status
      progress&.update_message("Sending request to Gemini API")
      
      response = http.request(request)
      
      # Update status
      progress&.update_message("Processing Gemini response")
      
      # Stop progress indicator
      progress&.stop
      
      if response.body.include?('API_KEY_INVALID')
        raise 'Invalid Gemini API key. Check your credentials.'
      end

      data = JSON.parse(response.body)
      
      # Handle error responses
      if data['error']
        raise "Gemini error: #{data['error']['message'] || data['error']}"
      end
      
      # Safely access the content
      content = data.dig('candidates', 0, 'content', 'parts', 0, 'text')
      unless content
        raise "Unexpected response format from Gemini. Response: #{data.inspect}"
      end
      
      # Process tool calls in the response
      content = process_tool_calls(content) if @tool_manager
      
      content
    ensure
      # Make sure progress indicator is stopped
      progress&.stop
    end
  end

  private

  def enhance_messages_with_tools(messages)
    enhanced_messages = messages.dup
    
    if @tool_manager && !enhanced_messages.empty?
      tools_context = @tool_manager.get_tools_context
      system_message = {
        role: 'system',
        content: "You are an AI assistant with access to various tools for file operations and system commands. #{tools_context}\n\nWhen you want to use a tool, include a TOOL_CALL in your response. You can make multiple tool calls in a single response. After each tool call, I will provide the result, and you can continue your response or make additional tool calls as needed.\n\nAlways explain what you're doing and why before using tools. Be helpful and thorough in your responses."
      }
      
      # Insert system message at the beginning if it doesn't exist, or merge with existing system message
      if enhanced_messages[0] && enhanced_messages[0][:role] == 'system'
        enhanced_messages[0][:content] = system_message[:content] + "\n\n" + enhanced_messages[0][:content]
      else
        enhanced_messages.unshift(system_message)
      end
    end
    
    enhanced_messages
  end

  def process_tool_calls(content)
    processed_content = content.dup
    
    content.scan(/TOOL_CALL:\s*([^\n]+)/) do |tool_call_match|
      tool_call = tool_call_match[0].strip
      tool_name, *args = tool_call.split(' ')
      
      begin
        puts "\n🔧 Tool requested: #{tool_name}"
        puts "   📥 Processing tool call in response..."
        $stdout.flush
        
        result = @tool_manager.execute_tool(tool_name, args.join(' '))
        
        # Replace the tool call with the result
        tool_call_line = "TOOL_CALL: #{tool_call}"
        replacement = "#{tool_call_line}\n\nTool Result:\n```\n#{result}\n```\n"
        processed_content = processed_content.gsub(tool_call_line, replacement)
        
        puts "   ✅ Tool completed: #{tool_name}"
        # Format tool output for better readability
        formatted_output = Lantae::ResponseFormatter.format_tool_output(tool_name, result, true)
        puts formatted_output
      rescue => error
        error_msg = "Error executing #{tool_name}: #{error.message}"
        puts "   ❌ Tool failed: #{tool_name}"
        puts "   🚫 Error: #{error.message}"
        
        tool_call_line = "TOOL_CALL: #{tool_call}"
        replacement = "#{tool_call_line}\n\nTool Error:\n```\n#{error_msg}\n```\n"
        processed_content = processed_content.gsub(tool_call_line, replacement)
      end
    end
    
    processed_content
  end

  def list_models
    %w[gemini-1.5-pro gemini-1.5-flash gemini-1.0-pro]
  end
end

class MistralProvider
  def initialize(secret_manager)
    @secret_manager = secret_manager
    @tool_manager = nil
  end

  def set_tool_manager(tool_manager)
    @tool_manager = tool_manager
  end

  def chat(model, messages, options = {})
    api_key = @secret_manager.get_api_key('mistral')
    raise 'Mistral API key not found in environment or AWS Secrets Manager' unless api_key

    # Add tool context to the system message
    enhanced_messages = enhance_messages_with_tools(messages)

    uri = URI('https://api.mistral.ai/v1/chat/completions')
    http = Net::HTTP.new(uri.host, uri.port)
    http.use_ssl = true

    request = Net::HTTP::Post.new(uri)
    request['Authorization'] = "Bearer #{api_key}"
    request['Content-Type'] = 'application/json'
    request.body = {
      model: model,
      messages: enhanced_messages,
      temperature: (options[:temperature] || 0.1).to_f,
      max_tokens: 4096
    }.to_json

    # Show progress indicator
    progress = ProgressIndicator.new("Preparing Mistral request")
    progress.start unless options[:no_spinner]
    
    begin
      # Update status
      progress&.update_message("Sending request to Mistral API")
      
      response = http.request(request)
      
      # Update status
      progress&.update_message("Processing Mistral response")
      
      # Stop progress indicator
      progress&.stop
      
      if response.code == '401'
        raise 'Invalid Mistral API key. Check your credentials.'
      end

      data = JSON.parse(response.body)
      
      # Handle error responses
      if data['error']
        raise "Mistral error: #{data['error']['message'] || data['error']}"
      end
      
      # Safely access the content
      content = data.dig('choices', 0, 'message', 'content')
      unless content
        raise "Unexpected response format from Mistral. Response: #{data.inspect}"
      end
      
      # Process tool calls in the response
      content = process_tool_calls(content) if @tool_manager
      
      content
    ensure
      # Make sure progress indicator is stopped
      progress&.stop
    end
  end

  private

  def enhance_messages_with_tools(messages)
    enhanced_messages = messages.dup
    
    if @tool_manager && !enhanced_messages.empty?
      tools_context = @tool_manager.get_tools_context
      system_message = {
        role: 'system',
        content: "You are an AI assistant with access to various tools for file operations and system commands. #{tools_context}\n\nWhen you want to use a tool, include a TOOL_CALL in your response. You can make multiple tool calls in a single response. After each tool call, I will provide the result, and you can continue your response or make additional tool calls as needed.\n\nAlways explain what you're doing and why before using tools. Be helpful and thorough in your responses."
      }
      
      # Insert system message at the beginning if it doesn't exist, or merge with existing system message
      if enhanced_messages[0] && enhanced_messages[0][:role] == 'system'
        enhanced_messages[0][:content] = system_message[:content] + "\n\n" + enhanced_messages[0][:content]
      else
        enhanced_messages.unshift(system_message)
      end
    end
    
    enhanced_messages
  end

  def process_tool_calls(content)
    processed_content = content.dup
    
    content.scan(/TOOL_CALL:\s*([^\n]+)/) do |tool_call_match|
      tool_call = tool_call_match[0].strip
      tool_name, *args = tool_call.split(' ')
      
      begin
        puts "\n🔧 Tool requested: #{tool_name}"
        puts "   📥 Processing tool call in response..."
        $stdout.flush
        
        result = @tool_manager.execute_tool(tool_name, args.join(' '))
        
        # Replace the tool call with the result
        tool_call_line = "TOOL_CALL: #{tool_call}"
        replacement = "#{tool_call_line}\n\nTool Result:\n```\n#{result}\n```\n"
        processed_content = processed_content.gsub(tool_call_line, replacement)
        
        puts "   ✅ Tool completed: #{tool_name}"
        # Format tool output for better readability
        formatted_output = Lantae::ResponseFormatter.format_tool_output(tool_name, result, true)
        puts formatted_output
      rescue => error
        error_msg = "Error executing #{tool_name}: #{error.message}"
        puts "   ❌ Tool failed: #{tool_name}"
        puts "   🚫 Error: #{error.message}"
        
        tool_call_line = "TOOL_CALL: #{tool_call}"
        replacement = "#{tool_call_line}\n\nTool Error:\n```\n#{error_msg}\n```\n"
        processed_content = processed_content.gsub(tool_call_line, replacement)
      end
    end
    
    processed_content
  end

  def list_models
    %w[mistral-large-latest mistral-medium-latest mistral-small-latest open-mistral-7b open-mixtral-8x7b open-mixtral-8x22b]
  end
end

class PerplexityProvider
  def initialize(secret_manager)
    @secret_manager = secret_manager
    @tool_manager = nil
  end

  def set_tool_manager(tool_manager)
    @tool_manager = tool_manager
  end

  def chat(model, messages, options = {})
    api_key = @secret_manager.get_api_key('perplexity')
    raise 'Perplexity API key not found in environment or AWS Secrets Manager' unless api_key

    # Add tool context to the system message
    enhanced_messages = enhance_messages_with_tools(messages)

    uri = URI('https://api.perplexity.ai/chat/completions')
    http = Net::HTTP.new(uri.host, uri.port)
    http.use_ssl = true

    request = Net::HTTP::Post.new(uri)
    request['Authorization'] = "Bearer #{api_key}"
    request['Content-Type'] = 'application/json'
    request.body = {
      model: model,
      messages: enhanced_messages,
      temperature: (options[:temperature] || 0.1).to_f,
      max_tokens: 4096
    }.to_json

    # Show progress indicator
    progress = ProgressIndicator.new("Preparing Perplexity request")
    progress.start unless options[:no_spinner]
    
    begin
      # Update status
      progress&.update_message("Sending request to Perplexity API")
      
      response = http.request(request)
      
      # Update status
      progress&.update_message("Processing Perplexity response")
      
      # Stop progress indicator
      progress&.stop
      
      if response.code == '401'
        raise 'Invalid Perplexity API key. Check your credentials.'
      end

      data = JSON.parse(response.body)
      
      # Handle error responses
      if data['error']
        raise "Perplexity error: #{data['error']['message'] || data['error']}"
      end
      
      # Safely access the content
      content = data.dig('choices', 0, 'message', 'content')
      unless content
        raise "Unexpected response format from Perplexity. Response: #{data.inspect}"
      end
      
      # Process tool calls in the response
      content = process_tool_calls(content) if @tool_manager
      
      content
    ensure
      # Make sure progress indicator is stopped
      progress&.stop
    end
  end

  private

  def enhance_messages_with_tools(messages)
    enhanced_messages = messages.dup
    
    if @tool_manager && !enhanced_messages.empty?
      tools_context = @tool_manager.get_tools_context
      system_message = {
        role: 'system',
        content: "You are an AI assistant with access to various tools for file operations and system commands. #{tools_context}\n\nWhen you want to use a tool, include a TOOL_CALL in your response. You can make multiple tool calls in a single response. After each tool call, I will provide the result, and you can continue your response or make additional tool calls as needed.\n\nAlways explain what you're doing and why before using tools. Be helpful and thorough in your responses."
      }
      
      # Insert system message at the beginning if it doesn't exist, or merge with existing system message
      if enhanced_messages[0] && enhanced_messages[0][:role] == 'system'
        enhanced_messages[0][:content] = system_message[:content] + "\n\n" + enhanced_messages[0][:content]
      else
        enhanced_messages.unshift(system_message)
      end
    end
    
    enhanced_messages
  end

  def process_tool_calls(content)
    processed_content = content.dup
    
    content.scan(/TOOL_CALL:\s*([^\n]+)/) do |tool_call_match|
      tool_call = tool_call_match[0].strip
      tool_name, *args = tool_call.split(' ')
      
      begin
        puts "\n🔧 Tool requested: #{tool_name}"
        puts "   📥 Processing tool call in response..."
        $stdout.flush
        
        result = @tool_manager.execute_tool(tool_name, args.join(' '))
        
        # Replace the tool call with the result
        tool_call_line = "TOOL_CALL: #{tool_call}"
        replacement = "#{tool_call_line}\n\nTool Result:\n```\n#{result}\n```\n"
        processed_content = processed_content.gsub(tool_call_line, replacement)
        
        puts "   ✅ Tool completed: #{tool_name}"
        # Format tool output for better readability
        formatted_output = Lantae::ResponseFormatter.format_tool_output(tool_name, result, true)
        puts formatted_output
      rescue => error
        error_msg = "Error executing #{tool_name}: #{error.message}"
        puts "   ❌ Tool failed: #{tool_name}"
        puts "   🚫 Error: #{error.message}"
        
        tool_call_line = "TOOL_CALL: #{tool_call}"
        replacement = "#{tool_call_line}\n\nTool Error:\n```\n#{error_msg}\n```\n"
        processed_content = processed_content.gsub(tool_call_line, replacement)
      end
    end
    
    processed_content
  end

  def list_models
    %w[llama-3.1-sonar-large-128k-online llama-3.1-sonar-small-128k-online llama-3.1-sonar-large-128k-chat llama-3.1-sonar-small-128k-chat llama-3.1-8b-instruct llama-3.1-70b-instruct]
  end
end

class BedrockProvider
  def initialize(region = 'us-east-1')
    @region = region
    @client = nil
    @tool_manager = nil
  end

  def set_tool_manager(tool_manager)
    @tool_manager = tool_manager
  end

  def chat(model, messages, options = {})
    init_client

    # Add tool context to the system message
    enhanced_messages = enhance_messages_with_tools(messages)

    model_map = {
      'claude-3-sonnet' => 'anthropic.claude-3-sonnet-20240229-v1:0',
      'claude-3-haiku' => 'anthropic.claude-3-haiku-20240307-v1:0',
      'claude-3-opus' => 'anthropic.claude-3-opus-20240229-v1:0',
      'claude-3-5-sonnet' => 'anthropic.claude-3-5-sonnet-20240620-v1:0',
      'claude-3-5-haiku' => 'anthropic.claude-3-5-haiku-20241022-v1:0',
      'titan-text-g1-large' => 'amazon.titan-text-lite-v1',
      'titan-text-g1-express' => 'amazon.titan-text-express-v1'
    }

    bedrock_model_id = model_map[model] || model

    if bedrock_model_id.include?('anthropic.claude')
      body = {
        anthropic_version: 'bedrock-2023-05-31',
        max_tokens: 4096,
        temperature: (options[:temperature] || 0.1).to_f,
        messages: enhanced_messages
      }.to_json
    elsif bedrock_model_id.include?('amazon.titan')
      prompt = enhanced_messages.map { |m| "#{m[:role]}: #{m[:content]}" }.join("\n")
      body = {
        inputText: prompt,
        textGenerationConfig: {
          temperature: (options[:temperature] || 0.1).to_f,
          maxTokenCount: 4096
        }
      }.to_json
    else
      raise "Unsupported model format: #{bedrock_model_id}"
    end

    # Show progress indicator
    progress = ProgressIndicator.new("Preparing AWS Bedrock request")
    progress.start unless options[:no_spinner]
    
    begin
      # Update status
      progress&.update_message("Invoking #{bedrock_model_id} on AWS Bedrock")
      
      response = @client.invoke_model({
        model_id: bedrock_model_id,
        body: body
      })

      # Update status
      progress&.update_message("Processing Bedrock response")
      
      # Stop progress indicator
      progress&.stop
      
      response_body = JSON.parse(response.body.read)
      
      # Handle error responses
      if response_body['error']
        raise "Bedrock error: #{response_body['error']['message'] || response_body['error']}"
      end

      content = if bedrock_model_id.include?('anthropic.claude')
        response_body.dig('content', 0, 'text')
      elsif bedrock_model_id.include?('amazon.titan')
        response_body.dig('results', 0, 'outputText')
      end
      
      unless content
        raise "Unexpected response format from Bedrock. Response: #{response_body.inspect}"
      end

      # Process tool calls in the response
      content = process_tool_calls(content) if @tool_manager
      
      content
    ensure
      # Make sure progress indicator is stopped
      progress&.stop
    end
  rescue Aws::Errors::MissingCredentialsError
    raise 'AWS credentials not found. Configure AWS CLI or environment variables.'
  end

  def list_models
    %w[claude-3-5-sonnet claude-3-5-haiku claude-3-sonnet claude-3-haiku claude-3-opus titan-text-g1-large titan-text-g1-express]
  end

  private

  def init_client
    return if @client
    @client = Aws::BedrockRuntime::Client.new(region: @region)
  end

  def enhance_messages_with_tools(messages)
    enhanced_messages = messages.dup
    
    if @tool_manager && !enhanced_messages.empty?
      tools_context = @tool_manager.get_tools_context
      system_message = {
        role: 'system',
        content: "You are an AI assistant with access to various tools for file operations and system commands. #{tools_context}\n\nWhen you want to use a tool, include a TOOL_CALL in your response. You can make multiple tool calls in a single response. After each tool call, I will provide the result, and you can continue your response or make additional tool calls as needed.\n\nAlways explain what you're doing and why before using tools. Be helpful and thorough in your responses."
      }
      
      # Insert system message at the beginning if it doesn't exist, or merge with existing system message
      if enhanced_messages[0] && enhanced_messages[0][:role] == 'system'
        enhanced_messages[0][:content] = system_message[:content] + "\n\n" + enhanced_messages[0][:content]
      else
        enhanced_messages.unshift(system_message)
      end
    end
    
    enhanced_messages
  end

  def process_tool_calls(content)
    processed_content = content.dup
    
    content.scan(/TOOL_CALL:\s*([^\n]+)/) do |tool_call_match|
      tool_call = tool_call_match[0].strip
      tool_name, *args = tool_call.split(' ')
      
      begin
        puts "\n🔧 Tool requested: #{tool_name}"
        puts "   📥 Processing tool call in response..."
        $stdout.flush
        
        result = @tool_manager.execute_tool(tool_name, args.join(' '))
        
        # Replace the tool call with the result
        tool_call_line = "TOOL_CALL: #{tool_call}"
        replacement = "#{tool_call_line}\n\nTool Result:\n```\n#{result}\n```\n"
        processed_content = processed_content.gsub(tool_call_line, replacement)
        
        puts "   ✅ Tool completed: #{tool_name}"
        # Format tool output for better readability
        formatted_output = Lantae::ResponseFormatter.format_tool_output(tool_name, result, true)
        puts formatted_output
      rescue => error
        error_msg = "Error executing #{tool_name}: #{error.message}"
        puts "   ❌ Tool failed: #{tool_name}"
        puts "   🚫 Error: #{error.message}"
        
        tool_call_line = "TOOL_CALL: #{tool_call}"
        replacement = "#{tool_call_line}\n\nTool Error:\n```\n#{error_msg}\n```\n"
        processed_content = processed_content.gsub(tool_call_line, replacement)
      end
    end
    
    processed_content
  end
end

class ToolManager
  def initialize(mcp_manager = nil, lsp_client = nil, auto_approve_reads = true)
    @mcp_manager = mcp_manager
    @lsp_client = lsp_client
    @tools_used = []
    @files_accessed = []
    @auto_approve_reads = auto_approve_reads
  end
  
  attr_reader :tools_used, :files_accessed
  
  def execute_tool(tool_name, args)
    # Track tool usage for LSP auto-start
    @tools_used << tool_name unless @tools_used.include?(tool_name)
    
    # Track file access for code intelligence
    if %w[cat write_file edit_file create_file].include?(tool_name)
      file_path = args.split(' ').first
      @files_accessed << file_path if file_path
    end
    
    # Check if approval is needed based on tool type
    if requires_approval?(tool_name) && !ENV['LANTAE_AUTO_APPROVE_ALL']
      unless get_user_approval(tool_name, args)
        return "Tool execution cancelled by user"
      end
    end
    
    # Show execution status
    puts "\n🔧 Tool execution requested: #{tool_name}"
    puts "   📦 Preparing tool environment..."
    $stdout.flush
    
    # Create progress indicator for long-running tools
    progress = nil
    if %w[bash ruby python git bundle find].include?(tool_name)
      progress = ProgressIndicator.new("Executing #{tool_name} command")
      progress.start
    end
    
    begin
      # Auto-start LSP if beneficial and not running
      auto_start_lsp_if_beneficial
      
      # Check if this is an MCP tool (format: server__tool)
    if tool_name.include?('__') && @mcp_manager
      return execute_mcp_tool(tool_name, args)
    end
    
      result = case tool_name
      when 'bash'
        execute_bash(args)
      when 'ruby'
        execute_ruby(args)
      when 'python'
        execute_python(args)
      when 'ls'
        list_files(args.empty? ? '.' : args)
      when 'cat'
        read_file(args)
      when 'pwd'
        Dir.pwd
      when 'git'
        execute_bash("git #{args}")
      when 'bundle'
        execute_bash("bundle #{args}")
      when 'write_file'
        write_file(args)
      when 'edit_file'
        edit_file(args)
      when 'create_file'
        create_file(args)
      when 'delete_file'
        delete_file(args)
      when 'mkdir'
        make_directory(args)
      when 'find'
        find_files(args)
      else
        raise "Tool '#{tool_name}' not found. Available tools: #{list_available_tools.join(', ')}"
      end
      
      # Stop progress indicator if running
      progress&.stop
      
      # Display success indicator
      puts "✅ Tool completed: #{tool_name}"
      $stdout.flush
      
      result
    rescue => e
      # Stop progress indicator on error
      progress&.stop
      
      # Display error indicator
      puts "❌ Tool failed: #{tool_name}"
      puts "   Error: #{e.message}"
      $stdout.flush
      
      raise
    end
  end

  def get_tools_context
    <<~CONTEXT
      Available tools you can use:
      - bash <command>: Execute bash commands
      - cat <file>: Read file contents
      - write_file <file> <content>: Write content to a file
      - edit_file <file> <line_number> <new_content>: Edit a specific line in a file
      - create_file <file> [content]: Create a new file with optional content
      - delete_file <file>: Delete a file
      - mkdir <directory>: Create a directory
      - ls [directory]: List files in directory
      - find <pattern>: Find files matching pattern
      - pwd: Get current directory
      - git <command>: Execute git commands
      - bundle <command>: Execute bundle commands
      - ruby <code>: Execute Ruby code
      - python <code>: Execute Python code

      To use a tool, format your response like this:
      TOOL_CALL: tool_name arguments
      For example:
      TOOL_CALL: cat Gemfile
      TOOL_CALL: write_file hello.txt Hello World!
      TOOL_CALL: edit_file main.rb 5 puts "Updated line"
    CONTEXT
  end

  def list_available_tools
    builtin_tools = %w[bash ruby python ls cat pwd git bundle write_file edit_file create_file delete_file mkdir find]
    
    if @mcp_manager
      mcp_tools = @mcp_manager.get_available_tools.keys
      builtin_tools + mcp_tools
    else
      builtin_tools
    end
  end

  private

  def requires_approval?(tool_name)
    # Only require approval for tools that modify the system
    write_tools = %w[write_file edit_file create_file delete_file mkdir bash ruby python]
    
    # Check environment variable for auto-approval of reads
    if ENV['LANTAE_AUTO_APPROVE_READS'] == 'true'
      return write_tools.include?(tool_name)
    end
    
    # By default, don't require approval for read-only operations
    return false unless @auto_approve_reads
    
    write_tools.include?(tool_name)
  end
  
  def get_user_approval(tool_name, args)
    # Skip approval if running in non-interactive mode
    return true unless $stdin.tty?
    
    # Clear any existing output and save cursor position
    print "\r\033[K"
    
    puts "\n⚠️  Tool execution request:"
    puts "   Tool: #{tool_name}"
    puts "   Args: #{args}"
    print "   Approve? (y/N): "
    $stdout.flush
    
    # Disable echo temporarily to avoid LSP debug interference
    begin
      system("stty -echo") if RUBY_PLATFORM =~ /darwin|linux/
      response = $stdin.gets
      system("stty echo") if RUBY_PLATFORM =~ /darwin|linux/
      puts # New line after input
      
      return false unless response
      response.chomp.downcase == 'y' || response.chomp.downcase == 'yes'
    rescue Interrupt
      system("stty echo") if RUBY_PLATFORM =~ /darwin|linux/
      puts "\n❌ Cancelled"
      false
    end
  end

  def auto_start_lsp_if_beneficial
    # Only auto-start if LSP client is not already available
    return if @lsp_client
    
    # Check if LSP auto-start would be beneficial based on tool usage
    context = {
      current_directory: Dir.pwd,
      tools_used: @tools_used,
      files_accessed: @files_accessed
    }
    
    # Use AutoStarter to determine if LSP should be started
    if Lantae::LSP::AutoStarter.auto_start_if_beneficial(context)
      # Try to connect client if server started successfully
      begin
        if Lantae::LSP::ServerManager.running?
          @lsp_client = Lantae::LSP::Client.new
          @lsp_client.start
        end
      rescue => e
        # Silently handle LSP connection errors - don't interrupt tool execution
        @lsp_client = nil
      end
    end
  rescue => e
    # Silently handle any auto-start errors - don't interrupt tool execution
  end

  def execute_mcp_tool(tool_name, args)
    puts "Executing MCP tool: #{tool_name}"
    
    begin
      # Parse arguments for MCP tool call
      arguments = parse_mcp_arguments(args)
      
      result = @mcp_manager.call_mcp_tool(tool_name, arguments)
      
      if result[:success]
        format_mcp_result(result[:result])
      else
        "MCP Tool Error: #{result[:error][:message] || 'Unknown error'}"
      end
      
    rescue => e
      puts "MCP tool execution failed: #{e.message}"
      "Error executing MCP tool #{tool_name}: #{e.message}"
    end
  end
  
  def parse_mcp_arguments(args_string)
    # Simple argument parsing - could be enhanced for complex structures
    return {} if args_string.nil? || args_string.strip.empty?
    
    # Try to parse as JSON first
    begin
      JSON.parse(args_string)
    rescue JSON::ParserError
      # Fallback to simple key=value parsing
      args = {}
      args_string.split(' ').each do |pair|
        if pair.include?('=')
          key, value = pair.split('=', 2)
          args[key] = value
        end
      end
      args
    end
  end
  
  def format_mcp_result(result)
    if result.is_a?(Hash) && result[:content]
      # Handle MCP content format
      content_parts = result[:content].map do |item|
        case item[:type]
        when 'text'
          item[:text]
        when 'image'
          "[Image: #{item[:data] ? 'embedded' : item[:url]}]"
        else
          item.to_s
        end
      end
      content_parts.join("\n")
    else
      result.to_s
    end
  end

  def execute_bash(command)
    result = `#{command} 2>&1`
    $?.success? ? result : "Error: #{result}"
  rescue => e
    "Error: #{e.message}"
  end

  def execute_ruby(code)
    eval(code).to_s
  rescue => e
    "Error: #{e.message}"
  end

  def execute_python(code)
    temp_file = Tempfile.new(['lantae', '.py'])
    temp_file.write(code)
    temp_file.close
    
    result = `python3 #{temp_file.path} 2>&1`
    temp_file.unlink
    result
  rescue => e
    "Error: #{e.message}"
  end

  def list_files(dir)
    Dir.entries(dir).join("\n")
  rescue => e
    "Error: #{e.message}"
  end

  def read_file(file_path)
    File.read(file_path)
  rescue => e
    "Error: #{e.message}"
  end

  def write_file(args)
    parts = args.split(' ', 2)
    file_path = parts[0]
    content = parts[1] || ''
    
    File.write(file_path, content)
    
    # Notify LSP about file change
    if @lsp_client
      begin
        @lsp_client.open_file(file_path, content)
        # Get initial diagnostics
        @lsp_client.get_code_actions(file_path, 0, 0, 0, 0)
      rescue => e
        # LSP errors shouldn't break file operations
      end
    end
    
    "File #{file_path} written successfully"
  rescue => e
    "Error: #{e.message}"
  end

  def edit_file(args)
    parts = args.split(' ', 3)
    file_path = parts[0]
    line_number = parts[1].to_i
    new_content = parts[2] || ''
    
    lines = File.readlines(file_path)
    
    if line_number > 0 && line_number <= lines.length
      lines[line_number - 1] = new_content + "\n"
      File.write(file_path, lines.join)
      "Line #{line_number} in #{file_path} edited successfully"
    else
      "Error: Line number #{line_number} out of range"
    end
  rescue => e
    "Error: #{e.message}"
  end

  def create_file(args)
    parts = args.split(' ', 2)
    file_path = parts[0]
    content = parts[1] || ''
    
    if File.exist?(file_path)
      "Error: File #{file_path} already exists"
    else
      File.write(file_path, content)
      
      # Notify LSP about new file
      if @lsp_client
        begin
          @lsp_client.open_file(file_path, content)
          # For Lantae-generated files, offer AI enhancements
          if content.include?('Generated by Lantae AI')
            @lsp_client.get_code_actions(file_path, 0, 0, 0, 0)
          end
        rescue => e
          # LSP errors shouldn't break file operations
        end
      end
      
      "File #{file_path} created successfully"
    end
  rescue => e
    "Error: #{e.message}"
  end

  def delete_file(file_path)
    File.delete(file_path)
    "File #{file_path} deleted successfully"
  rescue => e
    "Error: #{e.message}"
  end

  def make_directory(dir_path)
    require 'fileutils'
    FileUtils.mkdir_p(dir_path)
    "Directory #{dir_path} created successfully"
  rescue => e
    "Error: #{e.message}"
  end

  def find_files(pattern)
    result = `find . -name "#{pattern}" 2>&1`
    result.empty? ? 'No files found' : result
  rescue => e
    "Error: #{e.message}"
  end
end

def send_single_prompt(prompt, options)
  secret_manager = SecretManager.new(options[:region], options[:secret])
  
  # Initialize MCP if enabled
  mcp_manager = nil
  if options[:enable_mcp]
    mcp_manager = MCPManager.new
    if mcp_manager.load_server_configs(options[:mcp_config])
      mcp_manager.discover_servers
      mcp_manager.connect_all_servers
    else
      mcp_manager = nil
    end
  end
  
  tool_manager = ToolManager.new(mcp_manager, nil)  # No LSP in non-REPL mode
  provider_manager = ProviderManager.new(secret_manager, tool_manager)
  
  if options[:provider] != 'ollama'
    provider_manager.switch_provider(options[:provider], options[:model])
  else
    provider_manager.current_model = options[:model]
  end
  
  # Handle agent mode
  if options[:agent_mode]
    execute_agent_task(prompt, provider_manager, tool_manager, options)
  else
    response = provider_manager.chat([{ role: 'user', content: prompt }], options)
    formatted_response = Lantae::ResponseFormatter.format_response(response, 
      boxed: true,
      markdown: true
    )
    puts formatted_response
  end
rescue => e
  puts "Error: #{e.message}"
  exit 1
end

def execute_agent_task(task_description, provider_manager, tool_manager, options)
  puts "#{"\e[96m"}🤖 Agent Mode: Planning and executing task...#{"\e[0m"}\n"
  
  # Initialize agent components
  task_analyzer = TaskAnalyzer.new
  task_database = TaskDatabase.new
  
  planning_agent = PlanningAgent.new(
    provider_manager, 
    tool_manager,
    task_analyzer: task_analyzer,
    logger: Logger.new(STDOUT)
  )
  
  execution_engine = ExecutionEngine.new(
    provider_manager,
    tool_manager,
    logger: Logger.new(STDOUT)
  )
  
  # Plan the task
  puts "#{"\e[93m"}📋 Creating execution plan...#{"\e[0m"}"
  task = planning_agent.plan_task(task_description)
  
  # Display the plan
  puts "\n#{"\e[92m"}📊 Execution Plan:#{"\e[0m"}"
  puts task.to_tree_string
  
  # Ask for confirmation unless auto-accept
  unless options[:auto_accept]
    print "\n#{"\e[94m"}Proceed with execution? (y/n): #{"\e[0m"}"
    response = gets.chomp.downcase
    unless response == 'y' || response == 'yes'
      puts "#{"\e[91m"}Execution cancelled.#{"\e[0m"}"
      return
    end
  end
  
  # Execute the plan
  puts "\n#{"\e[93m"}⚙️  Executing plan...#{"\e[0m"}"
  success = planning_agent.execute_plan(task, execution_engine)
  
  # Record execution results
  if task.execution_result
    task_database.record_task_execution(task, task.execution_result)
  end
  
  # Display results
  puts "\n#{"\e[92m"}📊 Execution Results:#{"\e[0m"}"
  puts task.to_tree_string
  
  if success
    puts "\n#{"\e[92m"}✅ Task completed successfully!#{"\e[0m"}"
  else
    puts "\n#{"\e[91m"}❌ Task failed. Check the execution tree for details.#{"\e[0m"}"
    
    # Show optimization suggestions
    suggestions = task_database.suggest_prompt_improvements(task_description)
    if suggestions.any?
      puts "\n#{"\e[93m"}💡 Suggestions for improvement:#{"\e[0m"}"
      suggestions.each do |suggestion|
        puts "  - #{suggestion[:recommendation]} (#{suggestion[:reason]})"
      end
    end
  end
end

def print_banner
  # Randomly select between ships and cats
  banner_type = rand(14) # 7 ships + 7 cats
  
  case banner_type
  when 0
    # Large Container Ship
    puts <<~BANNER
      #{"\e[96m"}
    ╔════════════════════════════════════════════════════════════════════════════════════════════════════════════╗
    ║  #{"\e[95m"}██╗      █████╗ ███╗   ██╗████████╗ █████╗ ███████╗#{"\e[96m"}                                      ▄▄▄▄▄▄▄▄▄▄▄▄ ║
    ║  #{"\e[95m"}██║     ██╔══██╗████╗  ██║╚══██╔══╝██╔══██╗██╔════╝#{"\e[96m"}                                     █ CONTAINER █▄║
    ║  #{"\e[95m"}██║     ███████║██╔██╗ ██║   ██║   ███████║█████╗#{"\e[96m"}                                       █▄▄▄▄▄▄▄▄▄▄▄█ ║
    ║  #{"\e[95m"}██║     ██╔══██║██║╚██╗██║   ██║   ██╔══██║██╔══╝#{"\e[96m"}                                       █┌─┬─┬─┬─┬─┐█ ║
    ║  #{"\e[95m"}███████╗██║  ██║██║ ╚████║   ██║   ██║  ██║███████╗#{"\e[96m"}                                      █├─┼─┼─┼─┼─┤█▄║
    ║  #{"\e[95m"}╚══════╝╚═╝  ╚═╝╚═╝  ╚═══╝   ╚═╝   ╚═╝  ╚═╝╚══════╝#{"\e[96m"}                              ═══════█└─┴─┴─┴─┴─┘██║
    ║                                                                                      ≈≈≈≈≈≈≈█▄▄▄▄▄▄▄▄▄▄▄██║
    ║  #{"\e[93m"}🚀 Multi-Provider LLM Interface v#{VERSION}#{"\e[96m"}                                            ≈≈≈≈≈≈≈≈≈≈≈≈≈≈≈≈≈≈≈≈║
    ║  #{"\e[92m"}⚡ Powered by Cogito Reasoning Model#{"\e[96m"}                                               ≈≈≈≈≈≈≈≈≈≈≈≈≈≈≈≈≈≈≈≈║
    ║  #{"\e[94m"}🔗 Ollama • OpenAI • Anthropic • Bedrock & More#{"\e[96m"}                                  ≈≈≈≈≈≈≈≈≈≈≈≈≈≈≈≈≈≈≈≈║
    ╚════════════════════════════════════════════════════════════════════════════════════════════════════════════╝
      #{"\e[90m"}by thelastmerrymaker | thelastmerrymaker.com#{"\e[0m"}
    BANNER
    
  when 1
    # Oil Tanker
    puts <<~BANNER
      #{"\e[96m"}
    ╔════════════════════════════════════════════════════════════════════════════════════════════════════════════╗
    ║  #{"\e[95m"}██╗      █████╗ ███╗   ██╗████████╗ █████╗ ███████╗#{"\e[96m"}                                       ╭──────────╮ ║
    ║  #{"\e[95m"}██║     ██╔══██╗████╗  ██║╚══██╔══╝██╔══██╗██╔════╝#{"\e[96m"}                                       │OIL TANKER│ ║
    ║  #{"\e[95m"}██║     ███████║██╔██╗ ██║   ██║   ███████║█████╗#{"\e[96m"}                                     ╭───┴──────────┴─╮║
    ║  #{"\e[95m"}██║     ██╔══██║██║╚██╗██║   ██║   ██╔══██║██╔══╝#{"\e[96m"}                                     │ ○ ○ ○ ○ ○ ○ ○ │║
    ║  #{"\e[95m"}███████╗██║  ██║██║ ╚████║   ██║   ██║  ██║███████╗#{"\e[96m"}                                    │ ○ ○ ○ ○ ○ ○ ○ │║
    ║  #{"\e[95m"}╚══════╝╚═╝  ╚═╝╚═╝  ╚═══╝   ╚═╝   ╚═╝  ╚═╝╚══════╝#{"\e[96m"}                              ══════╰─────────────────╯║
    ║                                                                                      ≈≈≈≈≈≈╲═══════════════╱║
    ║  #{"\e[93m"}🚀 Multi-Provider LLM Interface v#{VERSION}#{"\e[96m"}                                            ≈≈≈≈≈≈≈╲▄▄▄▄▄▄▄▄▄▄▄▄╱║
    ║  #{"\e[92m"}⚡ Powered by Cogito Reasoning Model#{"\e[96m"}                                               ≈≈≈≈≈≈≈≈≈≈≈≈≈≈≈≈≈≈≈≈║
    ║  #{"\e[94m"}🔗 Ollama • OpenAI • Anthropic • Bedrock & More#{"\e[96m"}                                  ≈≈≈≈≈≈≈≈≈≈≈≈≈≈≈≈≈≈≈≈║
    ╚════════════════════════════════════════════════════════════════════════════════════════════════════════════╝
      #{"\e[90m"}by thelastmerrymaker | thelastmerrymaker.com#{"\e[0m"}
    BANNER
    
  when 2
    # Cargo Ship with Cranes
    puts <<~BANNER
      #{"\e[96m"}
    ╔════════════════════════════════════════════════════════════════════════════════════════════════════════════╗
    ║  #{"\e[95m"}██╗      █████╗ ███╗   ██╗████████╗ █████╗ ███████╗#{"\e[96m"}                                    ╱|╲   ╱|╲   ╱|╲║
    ║  #{"\e[95m"}██║     ██╔══██╗████╗  ██║╚══██╔══╝██╔══██╗██╔════╝#{"\e[96m"}                                   ╱ | ╲ ╱ | ╲ ╱ | ╲║
    ║  #{"\e[95m"}██║     ███████║██╔██╗ ██║   ██║   ███████║█████╗#{"\e[96m"}                                    ╱  |  ╳  |  ╳  |  ╲║
    ║  #{"\e[95m"}██║     ██╔══██║██║╚██╗██║   ██║   ██╔══██║██╔══╝#{"\e[96m"}                                   ┌───┴──┴──┴──┴──┴───┐║
    ║  #{"\e[95m"}███████╗██║  ██║██║ ╚████║   ██║   ██║  ██║███████╗#{"\e[96m"}                                  │  CARGO CARRIER   │║
    ║  #{"\e[95m"}╚══════╝╚═╝  ╚═╝╚═╝  ╚═══╝   ╚═╝   ╚═╝  ╚═╝╚══════╝#{"\e[96m"}                            ══════│ □ □ □ □ □ □ □ □ │║
    ║                                                                                      ≈≈≈≈└─────────────────┘║
    ║  #{"\e[93m"}🚀 Multi-Provider LLM Interface v#{VERSION}#{"\e[96m"}                                            ≈≈≈≈≈╲▄▄▄▄▄▄▄▄▄▄▄▄▄╱║
    ║  #{"\e[92m"}⚡ Powered by Cogito Reasoning Model#{"\e[96m"}                                               ≈≈≈≈≈≈≈≈≈≈≈≈≈≈≈≈≈≈≈≈║
    ║  #{"\e[94m"}🔗 Ollama • OpenAI • Anthropic • Bedrock & More#{"\e[96m"}                                  ≈≈≈≈≈≈≈≈≈≈≈≈≈≈≈≈≈≈≈≈║
    ╚════════════════════════════════════════════════════════════════════════════════════════════════════════════╝
      #{"\e[90m"}by thelastmerrymaker | thelastmerrymaker.com#{"\e[0m"}
    BANNER
    
  when 3
    # Bulk Carrier
    puts <<~BANNER
      #{"\e[96m"}
    ╔════════════════════════════════════════════════════════════════════════════════════════════════════════════╗
    ║  #{"\e[95m"}██╗      █████╗ ███╗   ██╗████████╗ █████╗ ███████╗#{"\e[96m"}                                       ╔══════════╗ ║
    ║  #{"\e[95m"}██║     ██╔══██╗████╗  ██║╚══██╔══╝██╔══██╗██╔════╝#{"\e[96m"}                                       ║BULK CARGO║ ║
    ║  #{"\e[95m"}██║     ███████║██╔██╗ ██║   ██║   ███████║█████╗#{"\e[96m"}                                     ╔═══╬══════════╬═╗║
    ║  #{"\e[95m"}██║     ██╔══██║██║╚██╗██║   ██║   ██╔══██║██╔══╝#{"\e[96m"}                                     ║░░░░░░░░░░░░░░░║║
    ║  #{"\e[95m"}███████╗██║  ██║██║ ╚████║   ██║   ██║  ██║███████╗#{"\e[96m"}                                    ║▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓║║
    ║  #{"\e[95m"}╚══════╝╚═╝  ╚═╝╚═╝  ╚═══╝   ╚═╝   ╚═╝  ╚═╝╚══════╝#{"\e[96m"}                              ══════║═══════════════║║
    ║                                                                                      ≈≈≈≈≈≈║▄▄▄▄▄▄▄▄▄▄▄▄▄▄║║
    ║  #{"\e[93m"}🚀 Multi-Provider LLM Interface v#{VERSION}#{"\e[96m"}                                            ≈≈≈≈≈≈╚═══════════════╝║
    ║  #{"\e[92m"}⚡ Powered by Cogito Reasoning Model#{"\e[96m"}                                               ≈≈≈≈≈≈≈≈≈≈≈≈≈≈≈≈≈≈≈≈║
    ║  #{"\e[94m"}🔗 Ollama • OpenAI • Anthropic • Bedrock & More#{"\e[96m"}                                  ≈≈≈≈≈≈≈≈≈≈≈≈≈≈≈≈≈≈≈≈║
    ╚════════════════════════════════════════════════════════════════════════════════════════════════════════════╝
      #{"\e[90m"}by thelastmerrymaker | thelastmerrymaker.com#{"\e[0m"}
    BANNER
    
  when 4
    # Research Vessel
    puts <<~BANNER
      #{"\e[96m"}
    ╔════════════════════════════════════════════════════════════════════════════════════════════════════════════╗
    ║  #{"\e[95m"}██╗      █████╗ ███╗   ██╗████████╗ █████╗ ███████╗#{"\e[96m"}                                         ╱╲    ╱╲   ║
    ║  #{"\e[95m"}██║     ██╔══██╗████╗  ██║╚══██╔══╝██╔══██╗██╔════╝#{"\e[96m"}                                        ╱██╲  ╱██╲  ║
    ║  #{"\e[95m"}██║     ███████║██╔██╗ ██║   ██║   ███████║█████╗#{"\e[96m"}                                       ╔══════════════╗║
    ║  #{"\e[95m"}██║     ██╔══██║██║╚██╗██║   ██║   ██╔══██║██╔══╝#{"\e[96m"}                                       ║ R/V EXPLORER ║║
    ║  #{"\e[95m"}███████╗██║  ██║██║ ╚████║   ██║   ██║  ██║███████╗#{"\e[96m"}                                      ║ ◯ ◯ ◯ ◯ ◯ ◯ ║║
    ║  #{"\e[95m"}╚══════╝╚═╝  ╚═╝╚═╝  ╚═══╝   ╚═╝   ╚═╝  ╚═╝╚══════╝#{"\e[96m"}                              ════════╬══════════════╬║
    ║                                                                                      ≈≈≈≈≈≈≈≈╲▄▄▄▄▄▄▄▄▄▄▄╱║
    ║  #{"\e[93m"}🚀 Multi-Provider LLM Interface v#{VERSION}#{"\e[96m"}                                            ≈≈≈≈≈≈≈≈≈≈≈≈≈≈≈≈≈≈≈≈║
    ║  #{"\e[92m"}⚡ Powered by Cogito Reasoning Model#{"\e[96m"}                                               ≈≈≈≈≈≈≈≈≈≈≈≈≈≈≈≈≈≈≈≈║
    ║  #{"\e[94m"}🔗 Ollama • OpenAI • Anthropic • Bedrock & More#{"\e[96m"}                                  ≈≈≈≈≈≈≈≈≈≈≈≈≈≈≈≈≈≈≈≈║
    ╚════════════════════════════════════════════════════════════════════════════════════════════════════════════╝
      #{"\e[90m"}by thelastmerrymaker | thelastmerrymaker.com#{"\e[0m"}
    BANNER
    
  when 5
    # Ferry
    puts <<~BANNER
      #{"\e[96m"}
    ╔════════════════════════════════════════════════════════════════════════════════════════════════════════════╗
    ║  #{"\e[95m"}██╗      █████╗ ███╗   ██╗████████╗ █████╗ ███████╗#{"\e[96m"}                                      ╔═══════════╗ ║
    ║  #{"\e[95m"}██║     ██╔══██╗████╗  ██║╚══██╔══╝██╔══██╗██╔════╝#{"\e[96m"}                                      ║   FERRY   ║ ║
    ║  #{"\e[95m"}██║     ███████║██╔██╗ ██║   ██║   ███████║█████╗#{"\e[96m"}                                        ║ ▢ ▢ ▢ ▢ ▢ ║ ║
    ║  #{"\e[95m"}██║     ██╔══██║██║╚██╗██║   ██║   ██╔══██║██╔══╝#{"\e[96m"}                                     ╔══╬═══════════╬══╗║
    ║  #{"\e[95m"}███████╗██║  ██║██║ ╚████║   ██║   ██║  ██║███████╗#{"\e[96m"}                                    ║▓▓║ ▢ ▢ ▢ ▢ ▢ ║▓▓║║
    ║  #{"\e[95m"}╚══════╝╚═╝  ╚═╝╚═╝  ╚═══╝   ╚═╝   ╚═╝  ╚═╝╚══════╝#{"\e[96m"}                              ══════╬══╬═══════════╬══╬║
    ║                                                                                      ≈≈≈≈≈≈║▄▄║▄▄▄▄▄▄▄▄▄▄▄║▄▄║║
    ║  #{"\e[93m"}🚀 Multi-Provider LLM Interface v#{VERSION}#{"\e[96m"}                                            ≈≈≈≈≈≈╚══╩═══════════╩══╝║
    ║  #{"\e[92m"}⚡ Powered by Cogito Reasoning Model#{"\e[96m"}                                               ≈≈≈≈≈≈≈≈≈≈≈≈≈≈≈≈≈≈≈≈║
    ║  #{"\e[94m"}🔗 Ollama • OpenAI • Anthropic • Bedrock & More#{"\e[96m"}                                  ≈≈≈≈≈≈≈≈≈≈≈≈≈≈≈≈≈≈≈≈║
    ╚════════════════════════════════════════════════════════════════════════════════════════════════════════════╝
      #{"\e[90m"}by thelastmerrymaker | thelastmerrymaker.com#{"\e[0m"}
    BANNER
    
  when 6
    # Naval Destroyer
    puts <<~BANNER
      #{"\e[96m"}
    ╔════════════════════════════════════════════════════════════════════════════════════════════════════════════╗
    ║  #{"\e[95m"}██╗      █████╗ ███╗   ██╗████████╗ █████╗ ███████╗#{"\e[96m"}                                           ▄▄▄▄    ║
    ║  #{"\e[95m"}██║     ██╔══██╗████╗  ██║╚══██╔══╝██╔══██╗██╔════╝#{"\e[96m"}                                          ╱|DD|╲   ║
    ║  #{"\e[95m"}██║     ███████║██╔██╗ ██║   ██║   ███████║█████╗#{"\e[96m"}                                           ╱─┴──┴─╲  ║
    ║  #{"\e[95m"}██║     ██╔══██║██║╚██╗██║   ██║   ██╔══██║██╔══╝#{"\e[96m"}                                        ╔═══════════╗ ║
    ║  #{"\e[95m"}███████╗██║  ██║██║ ╚████║   ██║   ██║  ██║███████╗#{"\e[96m"}                                       ║▓▓▓▓▓▓▓▓▓▓▓║▄║
    ║  #{"\e[95m"}╚══════╝╚═╝  ╚═╝╚═╝  ╚═══╝   ╚═╝   ╚═╝  ╚═╝╚══════╝#{"\e[96m"}                              ════════╬═══════════╬═║
    ║                                                                                      ≈≈≈≈≈≈≈≈≈╲▄▄▄▄▄▄▄▄▄╱≈║
    ║  #{"\e[93m"}🚀 Multi-Provider LLM Interface v#{VERSION}#{"\e[96m"}                                            ≈≈≈≈≈≈≈≈≈≈≈≈≈≈≈≈≈≈≈≈║
    ║  #{"\e[92m"}⚡ Powered by Cogito Reasoning Model#{"\e[96m"}                                               ≈≈≈≈≈≈≈≈≈≈≈≈≈≈≈≈≈≈≈≈║
    ║  #{"\e[94m"}🔗 Ollama • OpenAI • Anthropic • Bedrock & More#{"\e[96m"}                                  ≈≈≈≈≈≈≈≈≈≈≈≈≈≈≈≈≈≈≈≈║
    ╚════════════════════════════════════════════════════════════════════════════════════════════════════════════╝
      #{"\e[90m"}by thelastmerrymaker | thelastmerrymaker.com#{"\e[0m"}
    BANNER
    
  when 7
    # Sitting Cat
    puts <<~BANNER
      #{"\e[96m"}
    ╔════════════════════════════════════════════════════════════════════════════════════════════════════════════╗
    ║  #{"\e[95m"}██╗      █████╗ ███╗   ██╗████████╗ █████╗ ███████╗#{"\e[96m"}                                        ╱╲___╱╲    ║
    ║  #{"\e[95m"}██║     ██╔══██╗████╗  ██║╚══██╔══╝██╔══██╗██╔════╝#{"\e[96m"}                                       (  o.o  )   ║
    ║  #{"\e[95m"}██║     ███████║██╔██╗ ██║   ██║   ███████║█████╗#{"\e[96m"}                                          (  >^<  )   ║
    ║  #{"\e[95m"}██║     ██╔══██║██║╚██╗██║   ██║   ██╔══██║██╔══╝#{"\e[96m"}                                         ╱|      |╲  ║
    ║  #{"\e[95m"}███████╗██║  ██║██║ ╚████║   ██║   ██║  ██║███████╗#{"\e[96m"}                                       (_|  ~~  |_) ║
    ║  #{"\e[95m"}╚══════╝╚═╝  ╚═╝╚═╝  ╚═══╝   ╚═╝   ╚═╝  ╚═╝╚══════╝#{"\e[96m"}                                        |______|   ║
    ║                                                                                                 ^^    ^^   ║
    ║  #{"\e[93m"}🚀 Multi-Provider LLM Interface v#{VERSION}#{"\e[96m"}                                                             ║
    ║  #{"\e[92m"}⚡ Powered by Cogito Reasoning Model#{"\e[96m"}                                                                ║
    ║  #{"\e[94m"}🔗 Ollama • OpenAI • Anthropic • Bedrock & More#{"\e[96m"}                                                   ║
    ╚════════════════════════════════════════════════════════════════════════════════════════════════════════════╝
      #{"\e[90m"}by thelastmerrymaker | thelastmerrymaker.com#{"\e[0m"}
    BANNER
    
  when 8
    # Stretching Cat
    puts <<~BANNER
      #{"\e[96m"}
    ╔════════════════════════════════════════════════════════════════════════════════════════════════════════════╗
    ║  #{"\e[95m"}██╗      █████╗ ███╗   ██╗████████╗ █████╗ ███████╗#{"\e[96m"}                                          ╱╲ ╱╲    ║
    ║  #{"\e[95m"}██║     ██╔══██╗████╗  ██║╚══██╔══╝██╔══██╗██╔════╝#{"\e[96m"}                                         ( ⌒.⌒ )   ║
    ║  #{"\e[95m"}██║     ███████║██╔██╗ ██║   ██║   ███████║█████╗#{"\e[96m"}                              ╱════════════(  ═══  )   ║
    ║  #{"\e[95m"}██║     ██╔══██║██║╚██╗██║   ██║   ██╔══██║██╔══╝#{"\e[96m"}                             (              )     (    ║
    ║  #{"\e[95m"}███████╗██║  ██║██║ ╚████║   ██║   ██║  ██║███████╗#{"\e[96m"}                             ╲____________╱  ~~  ╱    ║
    ║  #{"\e[95m"}╚══════╝╚═╝  ╚═╝╚═╝  ╚═══╝   ╚═╝   ╚═╝  ╚═╝╚══════╝#{"\e[96m"}                                         ^^    ^^     ║
    ║                                                                                                             ║
    ║  #{"\e[93m"}🚀 Multi-Provider LLM Interface v#{VERSION}#{"\e[96m"}                                                             ║
    ║  #{"\e[92m"}⚡ Powered by Cogito Reasoning Model#{"\e[96m"}                                                                ║
    ║  #{"\e[94m"}🔗 Ollama • OpenAI • Anthropic • Bedrock & More#{"\e[96m"}                                                   ║
    ╚════════════════════════════════════════════════════════════════════════════════════════════════════════════╝
      #{"\e[90m"}by thelastmerrymaker | thelastmerrymaker.com#{"\e[0m"}
    BANNER
    
  when 9
    # Playful Cat
    puts <<~BANNER
      #{"\e[96m"}
    ╔════════════════════════════════════════════════════════════════════════════════════════════════════════════╗
    ║  #{"\e[95m"}██╗      █████╗ ███╗   ██╗████████╗ █████╗ ███████╗#{"\e[96m"}                                           ╱╲ ╱╲   ║
    ║  #{"\e[95m"}██║     ██╔══██╗████╗  ██║╚══██╔══╝██╔══██╗██╔════╝#{"\e[96m"}                                          ( >.< )  ║
    ║  #{"\e[95m"}██║     ███████║██╔██╗ ██║   ██║   ███████║█████╗#{"\e[96m"}                                            ( >o< )> ║
    ║  #{"\e[95m"}██║     ██╔══██║██║╚██╗██║   ██║   ██╔══██║██╔══╝#{"\e[96m"}                                           <(     )  ║
    ║  #{"\e[95m"}███████╗██║  ██║██║ ╚████║   ██║   ██║  ██║███████╗#{"\e[96m"}                                           ( ~~~ )  ║
    ║  #{"\e[95m"}╚══════╝╚═╝  ╚═╝╚═╝  ╚═══╝   ╚═╝   ╚═╝  ╚═╝╚══════╝#{"\e[96m"}                                          ^^   ^^  ║
    ║                                                                                                 ◯         ║
    ║  #{"\e[93m"}🚀 Multi-Provider LLM Interface v#{VERSION}#{"\e[96m"}                                                             ║
    ║  #{"\e[92m"}⚡ Powered by Cogito Reasoning Model#{"\e[96m"}                                                                ║
    ║  #{"\e[94m"}🔗 Ollama • OpenAI • Anthropic • Bedrock & More#{"\e[96m"}                                                   ║
    ╚════════════════════════════════════════════════════════════════════════════════════════════════════════════╝
      #{"\e[90m"}by thelastmerrymaker | thelastmerrymaker.com#{"\e[0m"}
    BANNER
    
  when 10
    # Sleeping Cat
    puts <<~BANNER
      #{"\e[96m"}
    ╔════════════════════════════════════════════════════════════════════════════════════════════════════════════╗
    ║  #{"\e[95m"}██╗      █████╗ ███╗   ██╗████████╗ █████╗ ███████╗#{"\e[96m"}                                        ╱╲_____╱╲  ║
    ║  #{"\e[95m"}██║     ██╔══██╗████╗  ██║╚══██╔══╝██╔══██╗██╔════╝#{"\e[96m"}                                       (  -.- z )  ║
    ║  #{"\e[95m"}██║     ███████║██╔██╗ ██║   ██║   ███████║█████╗#{"\e[96m"}                                         (         )  ║
    ║  #{"\e[95m"}██║     ██╔══██║██║╚██╗██║   ██║   ██╔══██║██╔══╝#{"\e[96m"}                                        ╱           ╲ ║
    ║  #{"\e[95m"}███████╗██║  ██║██║ ╚████║   ██║   ██║  ██║███████╗#{"\e[96m"}                                      (  ~~~~~~~~~~  )║
    ║  #{"\e[95m"}╚══════╝╚═╝  ╚═╝╚═╝  ╚═══╝   ╚═╝   ╚═╝  ╚═╝╚══════╝#{"\e[96m"}                                      ╲____________╱ ║
    ║                                                                                               Z z z z      ║
    ║  #{"\e[93m"}🚀 Multi-Provider LLM Interface v#{VERSION}#{"\e[96m"}                                                             ║
    ║  #{"\e[92m"}⚡ Powered by Cogito Reasoning Model#{"\e[96m"}                                                                ║
    ║  #{"\e[94m"}🔗 Ollama • OpenAI • Anthropic • Bedrock & More#{"\e[96m"}                                                   ║
    ╚════════════════════════════════════════════════════════════════════════════════════════════════════════════╝
      #{"\e[90m"}by thelastmerrymaker | thelastmerrymaker.com#{"\e[0m"}
    BANNER
    
  when 11
    # Curious Cat
    puts <<~BANNER
      #{"\e[96m"}
    ╔════════════════════════════════════════════════════════════════════════════════════════════════════════════╗
    ║  #{"\e[95m"}██╗      █████╗ ███╗   ██╗████████╗ █████╗ ███████╗#{"\e[96m"}                                           ╱╲ ╱╲   ║
    ║  #{"\e[95m"}██║     ██╔══██╗████╗  ██║╚══██╔══╝██╔══██╗██╔════╝#{"\e[96m"}                                          ( o.o )? ║
    ║  #{"\e[95m"}██║     ███████║██╔██╗ ██║   ██║   ███████║█████╗#{"\e[96m"}                                            (  ?  )  ║
    ║  #{"\e[95m"}██║     ██╔══██║██║╚██╗██║   ██║   ██╔══██║██╔══╝#{"\e[96m"}                                           ╱|    |╲ ║
    ║  #{"\e[95m"}███████╗██║  ██║██║ ╚████║   ██║   ██║  ██║███████╗#{"\e[96m"}                                         (_| ~~ |_)║
    ║  #{"\e[95m"}╚══════╝╚═╝  ╚═╝╚═╝  ╚═══╝   ╚═╝   ╚═╝  ╚═╝╚══════╝#{"\e[96m"}                                          |____|  ║
    ║                                                                                                  ^^  ^^  ║
    ║  #{"\e[93m"}🚀 Multi-Provider LLM Interface v#{VERSION}#{"\e[96m"}                                                             ║
    ║  #{"\e[92m"}⚡ Powered by Cogito Reasoning Model#{"\e[96m"}                                                                ║
    ║  #{"\e[94m"}🔗 Ollama • OpenAI • Anthropic • Bedrock & More#{"\e[96m"}                                                   ║
    ╚════════════════════════════════════════════════════════════════════════════════════════════════════════════╝
      #{"\e[90m"}by thelastmerrymaker | thelastmerrymaker.com#{"\e[0m"}
    BANNER
    
  when 12
    # Prowling Cat
    puts <<~BANNER
      #{"\e[96m"}
    ╔════════════════════════════════════════════════════════════════════════════════════════════════════════════╗
    ║  #{"\e[95m"}██╗      █████╗ ███╗   ██╗████████╗ █████╗ ███████╗#{"\e[96m"}                                           ,-.     ║
    ║  #{"\e[95m"}██║     ██╔══██╗████╗  ██║╚══██╔══╝██╔══██╗██╔════╝#{"\e[96m"}                                          ( o.o)   ║
    ║  #{"\e[95m"}██║     ███████║██╔██╗ ██║   ██║   ███████║█████╗#{"\e[96m"}                                            (   ))   ║
    ║  #{"\e[95m"}██║     ██╔══██║██║╚██╗██║   ██║   ██╔══██║██╔══╝#{"\e[96m"}                                           /|   |╲  ║
    ║  #{"\e[95m"}███████╗██║  ██║██║ ╚████║   ██║   ██║  ██║███████╗#{"\e[96m"}                                      ___/ |___|╲__║
    ║  #{"\e[95m"}╚══════╝╚═╝  ╚═╝╚═╝  ╚═══╝   ╚═╝   ╚═╝  ╚═╝╚══════╝#{"\e[96m"}                                    (_______________║
    ║                                                                                              ^^         ^^ ║
    ║  #{"\e[93m"}🚀 Multi-Provider LLM Interface v#{VERSION}#{"\e[96m"}                                                             ║
    ║  #{"\e[92m"}⚡ Powered by Cogito Reasoning Model#{"\e[96m"}                                                                ║
    ║  #{"\e[94m"}🔗 Ollama • OpenAI • Anthropic • Bedrock & More#{"\e[96m"}                                                   ║
    ╚════════════════════════════════════════════════════════════════════════════════════════════════════════════╝
      #{"\e[90m"}by thelastmerrymaker | thelastmerrymaker.com#{"\e[0m"}
    BANNER
    
  when 13
    # Happy Cat
    puts <<~BANNER
      #{"\e[96m"}
    ╔════════════════════════════════════════════════════════════════════════════════════════════════════════════╗
    ║  #{"\e[95m"}██╗      █████╗ ███╗   ██╗████████╗ █████╗ ███████╗#{"\e[96m"}                                         ╱╲___╱╲   ║
    ║  #{"\e[95m"}██║     ██╔══██╗████╗  ██║╚══██╔══╝██╔══██╗██╔════╝#{"\e[96m"}                                        ( ^.^  )  ║
    ║  #{"\e[95m"}██║     ███████║██╔██╗ ██║   ██║   ███████║█████╗#{"\e[96m"}                                          (  ___  ) ║
    ║  #{"\e[95m"}██║     ██╔══██║██║╚██╗██║   ██║   ██╔══██║██╔══╝#{"\e[96m"}                                         ╱|╱     ╲|╲║
    ║  #{"\e[95m"}███████╗██║  ██║██║ ╚████║   ██║   ██║  ██║███████╗#{"\e[96m"}                                       (_| ~~~~~ |_║
    ║  #{"\e[95m"}╚══════╝╚═╝  ╚═╝╚═╝  ╚═══╝   ╚═╝   ╚═╝  ╚═╝╚══════╝#{"\e[96m"}                                        |_______|  ║
    ║                                                                                                ^^     ^^ ║
    ║  #{"\e[93m"}🚀 Multi-Provider LLM Interface v#{VERSION}#{"\e[96m"}                                                             ║
    ║  #{"\e[92m"}⚡ Powered by Cogito Reasoning Model#{"\e[96m"}                                                                ║
    ║  #{"\e[94m"}🔗 Ollama • OpenAI • Anthropic • Bedrock & More#{"\e[96m"}                                                   ║
    ╚════════════════════════════════════════════════════════════════════════════════════════════════════════════╝
      #{"\e[90m"}by thelastmerrymaker | thelastmerrymaker.com#{"\e[0m"}
    BANNER
  end
  
  puts
end

def setup_autocomplete(provider_manager, tool_manager, mcp_manager = nil)
  # Define slash commands
  slash_commands = %w[help model provider models tool tools mcp clear info env lsp agent squad task]
  
  # Define providers
  providers = %w[ollama openai anthropic bedrock gemini mistral perplexity]
  
  # Get available models (cached for performance)
  models = []
  begin
    models = provider_manager.list_models || []
    # Add some default models if list is empty
    if models.empty?
      models = %w[cogito:latest llama3:latest codellama:latest mistral:latest]
    end
  rescue => e
    # Fallback to default models if can't be fetched
    models = %w[cogito:latest llama3:latest codellama:latest mistral:latest]
  end
  
  # Get available tools
  tools = []
  begin
    tools = tool_manager.list_available_tools || []
  rescue => e
    # Fallback to basic tools
    tools = %w[bash cat ls pwd git write_file edit_file create_file delete_file mkdir find]
  end
  
  # Get MCP subcommands
  mcp_subcommands = %w[status health reload tools]
  
  # Get LSP subcommands
  lsp_subcommands = %w[start stop restart status analyze format]
  
  # Get agent subcommands
  agent_subcommands = %w[plan execute report history]
  
  # Completion proc
  comp = proc do |input|
    begin
      completions = []
      
      # Safely handle input
      return [] if input.nil? || input.empty?
      
      # Handle slash commands
      if input.start_with?('/')
        # Extract command and args
        parts = input[1..-1].split(' ', 2)
        command = parts[0] || ''
        args = parts[1] || ''
        
        if parts.length == 1
          # Complete slash command itself
          completions = slash_commands.select { |cmd| cmd.start_with?(command) }.map { |cmd| "/#{cmd}" }
        else
          # Complete command arguments
          case command
          when 'provider'
            if args.split(' ').length == 1
              completions = providers.select { |p| p.start_with?(args) }.map { |p| "/provider #{p}" }
            elsif args.split(' ').length == 2
              provider_name = args.split(' ')[0]
              model_start = args.split(' ')[1]
              completions = models.select { |m| m.start_with?(model_start) }.map { |m| "/provider #{provider_name} #{m}" }
            end
          when 'model'
            completions = models.select { |m| m.start_with?(args) }.map { |m| "/model #{m}" }
          when 'tool'
            if args.split(' ', 2).length == 1
              completions = tools.select { |t| t.start_with?(args) }.map { |t| "/tool #{t}" }
            else
              tool_name = args.split(' ', 2)[0]
              file_arg = args.split(' ', 2)[1] || ''
              # For file-based tools, complete file paths
              if %w[cat write_file edit_file create_file delete_file].include?(tool_name)
                begin
                  completions = Dir.glob("#{file_arg}*").map { |f| "/tool #{tool_name} #{f}" }
                rescue => e
                  # If glob fails, return empty array
                  completions = []
                end
              end
            end
          when 'mcp'
            if mcp_manager && args.split(' ').length == 1
              completions = mcp_subcommands.select { |sub| sub.start_with?(args) }.map { |sub| "/mcp #{sub}" }
            end
          when 'lsp'
            if args.split(' ').length == 1
              completions = lsp_subcommands.select { |sub| sub.start_with?(args) }.map { |sub| "/lsp #{sub}" }
            end
          when 'agent'
            if args.split(' ').length == 1
              completions = agent_subcommands.select { |sub| sub.start_with?(args) }.map { |sub| "/agent #{sub}" }
            end
          end
        end
      else
        # Non-slash command completions (file paths)
        begin
          completions = Dir.glob("#{input}*").select { |f| File.file?(f) || File.directory?(f) }
        rescue => e
          # If glob fails, return empty array
          completions = []
        end
      end
      
      # Ensure we always return an array
      completions || []
    rescue => e
      # If anything goes wrong, return empty array to prevent crashes
      []
    end
  end
  
  Readline.completion_proc = comp
  Readline.completion_append_character = ' '
  
  # Debug info if tab completion not working
  if ENV['DEBUG_TAB']
    puts "Tab completion setup complete."
    puts "Readline version: #{Readline::VERSION rescue 'Unknown'}"
    puts "Completion proc set: #{!Readline.completion_proc.nil?}"
    puts "Try typing '/' and pressing TAB"
  end
end

def start_repl(options)
  secret_manager = SecretManager.new(options[:region], options[:secret])
  
  # Initialize MCP if enabled
  mcp_manager = nil
  if options[:enable_mcp]
    mcp_manager = MCPManager.new
    if mcp_manager.load_server_configs(options[:mcp_config])
      mcp_manager.discover_servers
      mcp_manager.connect_all_servers
    else
      mcp_manager = nil
    end
  end
  
  # Initialize LSP client for enhanced code support
  lsp_client = nil
  if options[:enable_lsp]
    begin
      # Smart auto-start LSP server if it would be beneficial
      context = {
        current_directory: Dir.pwd,
        tools_used: [],
        files_accessed: []
      }
      
      if Lantae::LSP::AutoStarter.auto_start_if_beneficial(context.merge(port: options[:lsp_port]))
        puts "✅ LSP server available"
      end
      
      # Connect client if server is available
      if Lantae::LSP::ServerManager.running?
        lsp_client = Lantae::LSP::Client.new
        if lsp_client.start
          puts "🔌 Connected to LSP server for enhanced code intelligence"
        else
          puts "⚠️  Failed to connect to LSP server"
          lsp_client = nil
        end
      end
    rescue => e
      puts "⚠️  LSP initialization error: #{e.message}"
      lsp_client = nil
    end
  end
  
  tool_manager = ToolManager.new(mcp_manager, lsp_client)
  
  # Smart provider detection
  detection_result = Lantae::SmartProviderManager.initialize_with_detection(
    secret_manager, 
    tool_manager,
    interactive: true
  )
  
  provider_manager = detection_result[:provider_manager]
  detection_info = detection_result[:detection_info]
  
  # Override with command line options if specified
  if options[:provider] && options[:provider] != 'ollama'
    provider_manager.switch_provider(options[:provider], options[:model])
  elsif options[:model] && detection_info[:provider] == 'ollama'
    provider_manager.current_model = options[:model]
  end
  
  # Initialize additional managers
  conversation_manager = Lantae::ConversationManager.new
  template_manager = Lantae::PromptTemplateManager.new
  cost_tracker = Lantae::CostTracker.new
  plugin_system = Lantae::PluginSystem.new
  interactive_enhancements = Lantae::InteractiveEnhancements.new
  
  # Initialize intelligent router if enabled
  intelligent_router = nil
  if options[:intelligent_routing]
    routing_config_file = options[:routing_config] || File.expand_path('~/.lantae/routing_config.yml')
    intelligent_router = Lantae::IntelligentRouter.new(provider_manager, routing_config_file)
    puts "🧠 Intelligent routing enabled"
  end
  
  conversation = []
  
  print_banner unless options[:no_banner]
  
  info = provider_manager.get_provider_info
  puts "#{"\e[96m"}Provider: #{"\e[93m"}#{info[:provider]}#{"\e[96m"} | Model: #{"\e[92m"}#{info[:model]}#{"\e[0m"}"
  
  # Display active modes
  modes = []
  modes << "#{"\e[93m"}Auto-Accept#{"\e[0m"}" if options[:auto_accept]
  modes << "#{"\e[94m"}Planning Mode#{"\e[0m"}" if options[:planning_mode]
  modes << "#{"\e[95m"}Agent Mode#{"\e[0m"}" if options[:agent_mode]
  puts "#{"\e[96m"}Active Modes: #{modes.join(', ')}#{"\e[0m"}" unless modes.empty?
  
  puts "#{"\e[90m"}Type \"/help\" for commands, \"exit\" or \"quit\" to end#{"\e[0m"}"
  puts

  begin
    models = provider_manager.list_models
    if models.empty?
      puts '⚠️  No models found.'
    elsif !models.include?(provider_manager.current_model)
      puts "⚠️  Model \"#{provider_manager.current_model}\" not found. Available models:"
      models.first(10).each { |m| puts "  - #{m}" }
      puts "  ... and #{models.length - 10} more" if models.length > 10
      puts
    end
  rescue => e
    puts "Error checking models: #{e.message}"
  end
  
  # Check if enhanced UI mode is requested
  if options[:enhanced_ui]
    # Use the enhanced REPL with split-screen UI
    enhanced_repl = Lantae::EnhancedREPL.new(provider_manager, tool_manager, options)
    enhanced_repl.start
    return
  end
  
  # Set up autocomplete
  setup_autocomplete(provider_manager, tool_manager, mcp_manager)

  loop do
    begin
      input = Readline.readline('> ', true)
      break if input.nil? || input.strip == 'exit' || input.strip == 'quit'
      
      input = input.strip
      next if input.empty?
      
      if input.start_with?('/')
        puts "\n🎯 Processing slash command: #{input.split(' ').first}..."
        $stdout.flush
        
        extra_managers = {
          conversation_manager: conversation_manager,
          template_manager: template_manager,
          cost_tracker: cost_tracker,
          plugin_system: plugin_system,
          interactive_enhancements: interactive_enhancements,
          intelligent_router: intelligent_router
        }
        handle_slash_command(input, provider_manager, tool_manager, conversation, mcp_manager, options, extra_managers)
        next
      end
      
      # Handle intelligent routing if enabled
      if intelligent_router
        puts "\n🧠 Using intelligent routing..."
        $stdout.flush
        
        # Route through intelligent system
        routing_context = {
          conversation_history: conversation,
          available_tools: tool_manager.list_available_tools,
          current_directory: Dir.pwd,
          options: options
        }
        
        response = intelligent_router.route_prompt(input, routing_context)
        conversation << { role: 'user', content: input }
        conversation << { role: 'assistant', content: response }
      else
        # Traditional routing
        # Handle planning mode
        if options[:planning_mode] && !input.downcase.include?('execute') && !input.downcase.include?('proceed')
          puts "\n📋 Planning mode: Creating detailed plan..."
          $stdout.flush
          input = "Please create a detailed plan for: #{input}. Break it down into clear steps and ask for confirmation before proceeding."
        end
        
        conversation << { role: 'user', content: input }
        
        puts "\n📡 Sending message to AI..."
        $stdout.flush
        
        response = provider_manager.chat(conversation, options)
        
        puts "\n📩 Response received, processing..."
        $stdout.flush
        
        conversation << { role: 'assistant', content: response }
      end
      # Format the response for better readability
      formatted_response = Lantae::ResponseFormatter.format_response(response, 
        boxed: true,
        markdown: true
      )
      puts formatted_response
      
      # Auto-accept mode handling
      if options[:auto_accept] && (response.downcase.include?('would you like') || response.downcase.include?('shall i') || response.downcase.include?('proceed'))
        puts "#{"\e[93m"}[AUTO-ACCEPT] Automatically confirming action...#{"\e[0m"}\n"
        conversation << { role: 'user', content: 'Yes, please proceed.' }
        
        auto_response = provider_manager.chat(conversation, options)
        conversation << { role: 'assistant', content: auto_response }
        puts "#{auto_response}\n\n"
      end
      
    rescue Interrupt
      puts "\nGoodbye!"
      break
    rescue => e
      puts "Error: #{e.message}"
    end
  end
end

def handle_slash_command(input, provider_manager, tool_manager, conversation, mcp_manager = nil, options = {}, extra_managers = {})
  parts = input[1..-1].split(' ')
  command = parts[0]
  args = parts[1..-1].join(' ')

  case command
  when 'help'
    puts <<~HELP
      Available commands:
        /model <name>          - Switch to a different model
        /provider <name>       - Switch provider (ollama, openai, anthropic, bedrock, gemini, mistral, perplexity)
        /lsp <subcommand>      - LSP commands (start, stop, status, analyze, format)
        /models                - List available models for current provider
        /tool <name> <args>    - Execute a local tool
        /tools                 - List available tools
        /mcp <subcommand>      - MCP server management (status, health, tools, reload)
        /agent <subcommand>    - Agent management (plan, execute, report, history)
        /squad <subcommand>    - Squad deployment (create, deploy, status, list)
        /task <subcommand>     - Task management (assign, list)
        /conversation <cmd>    - Manage conversations (save, load, list, search, export)
        /template <cmd>        - Manage templates (save, use, list, snippet)
        /cost <cmd>            - Cost tracking (status, report, budget, export)
        /plugin <cmd>          - Plugin management (list, enable, disable, install)
        /routing <cmd>         - Routing configuration (show, set, reset, stats)
        /clear                 - Clear conversation history
        /info                  - Show current provider and model info
        /env                   - Show environment variables status
        /help                  - Show this help message
        
      Shortcuts:
        Ctrl+M - Toggle multiline mode
        Ctrl+E - Open in editor
        Ctrl+R - Search history
        Ctrl+K - Clear screen
    HELP

  when 'model'
    if args.empty?
      puts 'Usage: /model <model-name>'
    else
      provider_manager.current_model = args
      puts "Switched to model: #{args}"
    end

  when 'provider'
    if args.empty?
      puts 'Usage: /provider <provider-name> [model]'
    else
      provider, model = args.split(' ', 2)
      provider_manager.switch_provider(provider, model)
      info = provider_manager.get_provider_info
      puts "Switched to provider: #{info[:provider]}, model: #{info[:model]}"
    end

  when 'models'
    models = provider_manager.list_models
    puts 'Available models:'
    models.each { |m| puts "  - #{m}" }

  when 'tool'
    if args.empty?
      puts 'Usage: /tool <tool-name> <arguments>'
    else
      tool_name, *tool_args = args.split(' ')
      result = tool_manager.execute_tool(tool_name, tool_args.join(' '))
      puts result
    end

  when 'tools'
    tools = tool_manager.list_available_tools
    puts 'Available tools:'
    tools.each { |t| puts "  - #{t}" }

  when 'clear'
    conversation.clear
    puts 'Conversation cleared.'

  when 'info'
    info = provider_manager.get_provider_info
    puts "Provider: #{info[:provider]}"
    puts "Model: #{info[:model]}"

  when 'mcp'
    handle_mcp_command(args, mcp_manager)

  when 'lsp'
    handle_lsp_command(args, tool_manager.instance_variable_get(:@lsp_client), options)

  when 'agent'
    handle_agent_command(args, provider_manager, tool_manager, options)

  when 'squad'
    handle_squad_command(args, provider_manager, tool_manager, conversation, options)

  when 'task'
    handle_task_command(args, provider_manager, tool_manager, conversation, options)

  when 'conversation'
    handle_conversation_command(args, conversation, extra_managers[:conversation_manager], provider_manager)

  when 'template'
    handle_template_command(args, conversation, extra_managers[:template_manager], provider_manager, options)

  when 'cost'
    handle_cost_command(args, extra_managers[:cost_tracker], provider_manager)

  when 'plugin'
    handle_plugin_command(args, extra_managers[:plugin_system])

  when 'routing'
    handle_routing_command(args, extra_managers[:intelligent_router], provider_manager)

  when 'env'
    puts 'Environment Variables Status:'
    %w[openai anthropic gemini mistral perplexity].each do |provider|
      key = "#{provider.upcase}_API_KEY"
      puts "#{key}: #{ENV[key] ? '✓ Set' : '✗ Not set'}"
    end
    puts "AWS_PROFILE: #{ENV['AWS_PROFILE'] || 'default'}"
    puts "AWS_REGION: #{ENV['AWS_REGION'] || 'not set'}"

  else
    puts "Unknown command: /#{command}. Type /help for available commands."
  end
rescue => e
  puts "Error: #{e.message}"
end

def handle_lsp_command(args, lsp_client, options = {})
  parts = args.split(' ')
  subcommand = parts.first
  
  case subcommand
  when 'start'
    port = parts[1]&.to_i || options[:lsp_port] || 7777
    if Lantae::LSP::ServerManager.start(port: port)
      puts "✅ LSP server started on port #{port}"
    else
      puts "❌ Failed to start LSP server"
    end
    
  when 'stop'
    if Lantae::LSP::ServerManager.stop
      puts "✅ LSP server stopped"
    else
      puts "❌ Failed to stop LSP server"
    end
    
  when 'restart'
    port = parts[1]&.to_i || options[:lsp_port] || 7777
    if Lantae::LSP::ServerManager.restart(port: port)
      puts "✅ LSP server restarted on port #{port}"
    else
      puts "❌ Failed to restart LSP server"
    end
    
  when 'status'
    Lantae::LSP::ServerManager.status
    
    if lsp_client
      puts "\nClient Status: #{lsp_client.instance_variable_get(:@running) ? '✅ Connected' : '❌ Disconnected'}"
      if lsp_client.capabilities.any?
        puts "Client Capabilities:"
        lsp_client.capabilities.each do |cap, value|
          puts "  - #{cap}: #{value ? '✓' : '✗'}"
        end
      end
    else
      puts "\nClient Status: ❌ Not initialized"
    end
    
  when 'analyze'
    if lsp_client.nil?
      puts "❌ LSP client not connected. Use /lsp start first."
      return
    end
    
    file_path = parts[1]
    if file_path.nil? || file_path.empty?
      puts "Usage: /lsp analyze <file_path>"
      return
    end
    
    if File.exist?(file_path)
      content = File.read(file_path)
      lsp_client.open_file(file_path, content)
      # Wait a moment for diagnostics
      sleep(0.5)
      puts "✅ File analyzed: #{file_path}"
    else
      puts "❌ File not found: #{file_path}"
    end
    
  when 'format'
    if lsp_client.nil?
      puts "❌ LSP client not connected. Use /lsp start first."
      return
    end
    
    file_path = parts[1]
    if file_path.nil? || file_path.empty?
      puts "Usage: /lsp format <file_path>"
      return
    end
    
    if File.exist?(file_path)
      content = File.read(file_path)
      lsp_client.open_file(file_path, content)
      edits = lsp_client.format_document(file_path)
      
      if edits && !edits.empty?
        # Apply formatting edits
        edits.each do |edit|
          # Simple implementation - just replace the whole content
          if edit['newText']
            File.write(file_path, edit['newText'])
            puts "✅ File formatted: #{file_path}"
          end
        end
      else
        puts "ℹ️  No formatting changes needed"
      end
    else
      puts "❌ File not found: #{file_path}"
    end
    
  when 'complete'
    puts "ℹ️  Interactive completion not yet implemented. Use code completion in your editor."
    
  else
    puts <<~HELP
      LSP Commands:
        /lsp start [port]        - Start LSP server
        /lsp stop                - Stop LSP server  
        /lsp restart [port]      - Restart LSP server
        /lsp status              - Show server and client status
        /lsp analyze <file>      - Analyze a file for issues
        /lsp format <file>       - Format a file
        /lsp complete            - Get completions (coming soon)
    HELP
  end
end

def handle_mcp_command(args, mcp_manager)
  unless mcp_manager
    puts "MCP not enabled. Use --enable-mcp flag to enable MCP support."
    return
  end
  
  parts = args.split(' ', 2)
  subcommand = parts[0]
  
  case subcommand
  when 'status'
    status = mcp_manager.get_server_status
    
    puts "MCP Server Status:"
    puts "  Discovered: #{status[:discovered]}"
    puts "  Connected: #{status[:connected]}"
    puts
    
    status[:servers].each do |server_name, server_status|
      puts "  #{server_name}:"
      puts "    Status: #{server_status[:status]}"
      puts "    Transport: #{server_status[:transport]}"
      puts "    Tools: #{server_status[:tools_count]}"
      puts "    Resources: #{server_status[:resources_count]}"
      
      if server_status[:last_error]
        puts "    Last Error: #{server_status[:last_error]}"
      end
      
      puts
    end
    
  when 'health'
    health = mcp_manager.health_check
    
    puts "MCP Health Check Results:"
    puts "  Total Servers: #{health[:total_servers]}"
    puts "  Healthy: #{health[:healthy_servers]}"
    puts "  Unhealthy: #{health[:unhealthy_servers]}"
    puts "  Health Rate: #{health[:health_rate]}%"
    
    unless health[:issues].empty?
      puts "\n  Issues:"
      health[:issues].each do |issue|
        puts "    #{issue[:server]}: #{issue[:error]}"
      end
    end
    
  when 'reload'
    puts "Reloading MCP configuration..."
    if mcp_manager.reload_configuration
      puts "MCP configuration reloaded successfully."
      
      status = mcp_manager.get_server_status
      puts "Connected to #{status[:connected]} servers."
    else
      puts "Failed to reload MCP configuration."
    end
    
  when 'tools'
    tools = mcp_manager.get_available_tools
    
    if tools.empty?
      puts "No MCP tools available."
    else
      puts "Available MCP Tools:"
      tools.each do |qualified_name, tool_info|
        puts "  #{qualified_name}"
        puts "    Server: #{tool_info[:server]}"
        
        if tool_info[:info][:description]
          puts "    Description: #{tool_info[:info][:description]}"
        end
        
        if tool_info[:info][:inputSchema]
          puts "    Parameters: #{tool_info[:info][:inputSchema][:properties]&.keys&.join(', ') || 'None'}"
        end
        
        puts
      end
    end
    
  when '', nil
    puts "MCP subcommands: status, health, reload, tools"
    
  else
    puts "Unknown MCP subcommand: #{subcommand}. Available: status, health, reload, tools"
  end
  
rescue => e
  puts "MCP command error: #{e.message}"
end

def handle_agent_command(args, provider_manager, tool_manager, options)
  parts = args.split(' ', 2)
  subcommand = parts[0]
  task_description = parts[1]
  
  case subcommand
  when 'plan'
    if task_description.nil? || task_description.empty?
      puts "Usage: /agent plan <task description>"
      return
    end
    
    # Just create and show the plan
    task_analyzer = TaskAnalyzer.new
    planning_agent = PlanningAgent.new(provider_manager, tool_manager, task_analyzer: task_analyzer)
    
    puts "#{"\e[93m"}📋 Creating execution plan...#{"\e[0m"}"
    task = planning_agent.plan_task(task_description)
    
    puts "\n#{"\e[92m"}📊 Execution Plan:#{"\e[0m"}"
    puts task.to_tree_string
    
  when 'execute'
    if task_description.nil? || task_description.empty?
      puts "Usage: /agent execute <task description>"
      return
    end
    
    # Full agent execution
    execute_agent_task(task_description, provider_manager, tool_manager, options)
    
  when 'report'
    # Show task execution report
    task_database = TaskDatabase.new
    puts task_database.generate_optimization_report
    
  when 'history'
    # Show recent task history
    task_database = TaskDatabase.new
    recent_tasks = task_database.get_common_failures(10)
    
    puts "#{"\e[92m"}Recent Task History:#{"\e[0m"}"
    puts "-" * 50
    
    recent_tasks.each_with_index do |task, i|
      puts "#{i + 1}. #{task[:description][0..60]}#{'...' if task[:description].length > 60}"
      puts "   Failures: #{task[:failure_count]}"
      puts "   Issues: #{task[:common_issues].join(', ')}" if task[:common_issues].any?
      puts
    end
    
  when '', nil
    puts "Agent subcommands: plan, execute, report, history"
    puts "Example: /agent plan Create a REST API"
    
  else
    puts "Unknown agent subcommand: #{subcommand}"
    puts "Available: plan, execute, report, history"
  end
  
rescue => e
  puts "Agent command error: #{e.message}"
  puts e.backtrace.first(5).join("\n") if ENV['DEBUG']
end

def handle_squad_command(args, provider_manager, tool_manager, conversation, options)
  # Load squad deployment class
  require_relative '../lib/ruby/squad_deployment'
  
  # Initialize context with squads storage
  conversation.instance_variable_set(:@squads, {}) unless conversation.instance_variable_defined?(:@squads)
  squads = conversation.instance_variable_get(:@squads)
  
  parts = args.split(' ', 2)
  subcommand = parts[0]
  
  case subcommand
  when 'create'
    squad_name = parts[1]
    
    if squad_name.nil? || squad_name.empty?
      puts "Usage: /squad create <squad_name>"
      return
    end
    
    squad = Lantae::SquadDeployment.new(
      squad_name,
      provider_manager: provider_manager,
      tool_manager: tool_manager,
      execution_engine: ExecutionEngine.new(provider_manager, tool_manager)
    )
    
    squads[squad_name] = squad
    
    puts "✅ Squad '#{squad_name}' created"
    
    # Add default members
    puts "Adding default squad members..."
    
    squad.add_member(
      name: "Planner",
      role: :planner,
      capabilities: [:planning, :task_decomposition],
      model: provider_manager.current_model
    )
    
    squad.add_member(
      name: "Executor-1", 
      role: :executor,
      capabilities: [:coding, :advanced_coding],
      model: provider_manager.current_model
    )
    
    squad.add_member(
      name: "Reviewer",
      role: :reviewer,
      capabilities: [:code_review, :quality_assurance],
      model: provider_manager.current_model
    )
    
    puts "Squad '#{squad_name}' is ready with #{squad.members.size} members"
    
  when 'deploy'
    squad_name = parts[1]
    
    if squad_name.nil? || squad_name.empty?
      puts "Usage: /squad deploy <squad_name>"
      return
    end
    
    squad = squads[squad_name]
    
    if squad.nil?
      puts "❌ Squad '#{squad_name}' not found. Create it first with: /squad create #{squad_name}"
      return
    end
    
    if squad.tasks.empty?
      puts "❌ No tasks assigned to squad. Use /task assign to add tasks."
      return
    end
    
    puts "🚀 Deploying squad '#{squad_name}'..."
    
    begin
      squad.deploy
      puts "✅ Squad deployment completed!"
      
      # Show results
      report = squad.get_status_report
      display_squad_report(report)
      
    rescue => e
      puts "❌ Deployment failed: #{e.message}"
    end
    
  when 'status'
    if parts[1].nil? || parts[1].empty?
      # Show all squads
      if squads.empty?
        puts "No squads created yet."
      else
        squads.each do |name, squad|
          puts "\n#{name}:"
          display_squad_report(squad.get_status_report)
        end
      end
    else
      squad_name = parts[1]
      squad = squads[squad_name]
      
      if squad.nil?
        puts "❌ Squad '#{squad_name}' not found"
      else
        display_squad_report(squad.get_status_report)
      end
    end
    
  when 'list'
    if squads.empty?
      puts "No squads created yet."
    else
      puts "Available squads:"
      squads.each do |name, squad|
        status_icon = case squad.status
                     when :deployed then "✅"
                     when :deploying then "🔄"
                     when :failed then "❌"
                     else "⏸️"
                     end
        
        puts "  #{status_icon} #{name} (#{squad.members.size} members, #{squad.tasks.size} tasks)"
      end
    end
    
  else
    puts <<~HELP
      Squad Commands:
        /squad create <name>     - Create a new squad
        /squad deploy <name>     - Deploy a squad to execute tasks
        /squad status [name]     - Show squad status
        /squad list             - List all squads
        
      Use /task assign to add tasks to a squad
    HELP
  end
  
rescue => e
  puts "Squad command error: #{e.message}"
  puts e.backtrace.first(5).join("\n") if ENV['DEBUG']
end

def handle_task_command(args, provider_manager, tool_manager, conversation, options)
  # Ensure squads are initialized
  conversation.instance_variable_set(:@squads, {}) unless conversation.instance_variable_defined?(:@squads)
  squads = conversation.instance_variable_get(:@squads)
  
  parts = args.split(' ', 3)
  subcommand = parts[0]
  
  case subcommand
  when 'assign'
    squad_name = parts[1]
    task_description = parts[2]
    
    if squad_name.nil? || task_description.nil? || task_description.empty?
      puts "Usage: /task assign <squad_name> <task_description>"
      return
    end
    
    squad = squads[squad_name]
    
    if squad.nil?
      puts "❌ Squad '#{squad_name}' not found. Create it first with: /squad create #{squad_name}"
      return
    end
    
    # Parse priority if specified
    priority = :normal
    if task_description =~ /\[priority:(high|low|urgent)\]/i
      priority = $1.downcase.to_sym
      task_description.gsub!(/\[priority:\w+\]/i, '').strip
    end
    
    # Assign task
    task = squad.assign_task(task_description, priority: priority)
    
    puts "✅ Task assigned to #{task[:assigned_to][:name]} in squad '#{squad_name}'"
    puts "   Priority: #{priority}"
    puts "   Task ID: #{task[:id]}"
    
  when 'list'
    squad_name = parts[1]
    
    if squad_name.nil?
      # List all tasks
      if squads.empty?
        puts "No squads created yet."
      else
        squads.each do |name, squad|
          if squad.tasks.any?
            puts "\n#{name} tasks:"
            display_squad_tasks(squad)
          end
        end
      end
    else
      squad = squads[squad_name]
      
      if squad.nil?
        puts "❌ Squad '#{squad_name}' not found"
      else
        if squad.tasks.empty?
          puts "No tasks assigned to squad '#{squad_name}'"
        else
          puts "Tasks for squad '#{squad_name}':"
          display_squad_tasks(squad)
        end
      end
    end
    
  else
    puts <<~HELP
      Task Commands:
        /task assign <squad> <description>  - Assign a task to a squad
        /task list [squad]                 - List tasks (all or for specific squad)
        
      Priority levels (optional):
        Add [priority:urgent], [priority:high], or [priority:low] to task description
        
      Examples:
        /task assign dev-team Implement login feature
        /task assign dev-team Fix bug in payment [priority:urgent]
    HELP
  end
  
rescue => e
  puts "Task command error: #{e.message}"
  puts e.backtrace.first(5).join("\n") if ENV['DEBUG']
end

def display_squad_report(report)
  puts "  Status: #{report[:status]}"
  puts "  Completion: #{report[:completion_rate]}%"
  
  puts "  Members:"
  report[:members].each do |member|
    status_icon = member[:status] == :busy ? "🔄" : "✅"
    puts "    #{status_icon} #{member[:name]} (#{member[:role]})"
  end
  
  if report[:tasks].any?
    puts "  Tasks:"
    report[:tasks].each do |task|
      status_icon = case task[:status]
                   when :completed then "✅"
                   when :in_progress then "🔄"
                   when :failed then "❌"
                   else "⏸️"
                   end
      
      puts "    #{status_icon} #{task[:description]} → #{task[:assigned_to]}"
    end
  end
end

def display_squad_tasks(squad)
  squad.tasks.each_with_index do |task, index|
    status_icon = case task[:status]
                 when :completed then "✅"
                 when :in_progress then "🔄"
                 when :failed then "❌"
                 else "⏸️"
                 end
    
    priority_icon = case task[:priority]
                   when :urgent then "🔴"
                   when :high then "🟡"
                   when :low then "🟢"
                   else "⚪"
                   end
    
    puts "  #{index + 1}. #{status_icon} #{priority_icon} #{task[:description]}"
    puts "     Assigned to: #{task[:assigned_to][:name]}"
    puts "     Status: #{task[:status]}"
    
    if task[:error]
      puts "     Error: #{task[:error]}"
    elsif task[:completed_at]
      duration = (task[:completed_at] - task[:created_at]).round(2)
      puts "     Completed in: #{duration}s"
    end
  end
end

def handle_conversation_command(args, conversation, conversation_manager, provider_manager)
  # Use command registry if available, otherwise fallback
  cmd = Lantae::CLI::Commands::ConversationCommand.new
  context = {
    conversation: conversation,
    provider_manager: provider_manager,
    current_session: Thread.current[:current_session]
  }
  cmd.execute(args.split(' '), context)
rescue => e
  puts "Conversation command error: #{e.message}"
end

def handle_template_command(args, conversation, template_manager, provider_manager, options)
  cmd = Lantae::CLI::Commands::TemplateCommand.new
  context = {
    conversation: conversation,
    provider_manager: provider_manager,
    options: options
  }
  cmd.execute(args.split(' '), context)
rescue => e
  puts "Template command error: #{e.message}"
end

def handle_cost_command(args, cost_tracker, provider_manager)
  cmd = Lantae::CLI::Commands::CostCommand.new
  context = {
    provider_manager: provider_manager
  }
  cmd.execute(args.split(' '), context)
rescue => e
  puts "Cost command error: #{e.message}"
end

def handle_plugin_command(args, plugin_system)
  parts = args.split(' ', 2)
  subcommand = parts[0]
  
  case subcommand
  when 'list'
    plugins = plugin_system.list_plugins
    if plugins.empty?
      puts "No plugins loaded."
    else
      puts "Loaded plugins:"
      plugins.each do |plugin|
        puts "  • #{plugin[:name]} v#{plugin[:version]}"
        puts "    #{plugin[:description]}"
        puts "    Author: #{plugin[:author]}"
        puts "    Hooks: #{plugin[:hooks].join(', ')}" if plugin[:hooks].any?
      end
    end
    
  when 'enable'
    plugin_name = parts[1]
    if plugin_name.nil? || plugin_name.empty?
      puts "Usage: /plugin enable <plugin_name>"
      return
    end
    
    if plugin_system.enable_plugin(plugin_name)
      puts "✅ Plugin '#{plugin_name}' enabled"
    else
      puts "❌ Plugin '#{plugin_name}' not found"
    end
    
  when 'disable'
    plugin_name = parts[1]
    if plugin_name.nil? || plugin_name.empty?
      puts "Usage: /plugin disable <plugin_name>"
      return
    end
    
    if plugin_system.disable_plugin(plugin_name)
      puts "✅ Plugin '#{plugin_name}' disabled"
    else
      puts "❌ Plugin '#{plugin_name}' not found"
    end
    
  when 'install'
    source = parts[1]
    if source.nil? || source.empty?
      puts "Usage: /plugin install <source>"
      puts "Source can be: URL, GitHub repo (user/repo), or local file path"
      return
    end
    
    begin
      plugin_system.install_plugin(source)
      puts "✅ Plugin installed from #{source}"
    rescue => e
      puts "❌ Failed to install plugin: #{e.message}"
    end
    
  else
    puts <<~HELP
      Plugin Commands:
        /plugin list              - List loaded plugins
        /plugin enable <name>     - Enable a plugin
        /plugin disable <name>    - Disable a plugin
        /plugin install <source>  - Install a plugin
        
      Install sources:
        - URL: https://example.com/plugin.rb
        - GitHub: username/repo/path/to/plugin.rb
        - Local: /path/to/plugin.rb
    HELP
  end
rescue => e
  puts "Plugin command error: #{e.message}"
end

def handle_routing_command(args, intelligent_router, provider_manager)
  parts = args.split(' ', 2)
  subcommand = parts[0]
  
  case subcommand
  when 'show'
    if intelligent_router
      Lantae::RoutingConfigManager.show_config
    else
      puts "Intelligent routing is not enabled. Run with --intelligent-routing flag."
    end
    
  when 'set'
    stage_args = parts[1]
    if stage_args.nil? || stage_args.empty?
      puts "Usage: /routing set <stage> <key>=<value> [key2=value2 ...]"
      puts "Stages: classifier, planning, squad, implementation, quick_answer, analysis"
      puts "Keys: provider, model, temperature, max_tokens"
      return
    end
    
    stage_parts = stage_args.split(' ')
    stage = stage_parts[0].to_sym
    
    # Parse key=value pairs
    settings = {}
    stage_parts[1..-1].each do |pair|
      key, value = pair.split('=')
      next unless key && value
      
      # Convert values to appropriate types
      case key
      when 'temperature'
        settings[key.to_sym] = value.to_f
      when 'max_tokens'
        settings[key.to_sym] = value.to_i
      else
        settings[key.to_sym] = value
      end
    end
    
    if settings.any?
      Lantae::RoutingConfigManager.update_stage_config(stage, **settings)
      
      # Reload router if it's active
      if intelligent_router
        routing_config_file = File.expand_path('~/.lantae/routing_config.yml')
        intelligent_router.instance_variable_set(:@routing_config, Lantae::RoutingConfigManager.load_or_create_config)
      end
    else
      puts "No valid settings provided"
    end
    
  when 'reset'
    Lantae::RoutingConfigManager.save_default_config
    puts "Routing configuration reset to defaults"
    
  when 'stats'
    if intelligent_router
      stats = intelligent_router.get_routing_stats
      puts "\nRouting Statistics:"
      puts "=" * 40
      puts "Total routes: #{stats[:total_routes]}"
      puts "\nBy prompt type:"
      stats[:by_type].each do |type, count|
        puts "  #{type}: #{count}"
      end
      puts "\nBy routing path:"
      stats[:by_path].each do |path, count|
        puts "  #{path}: #{count}"
      end
      puts "\nAverage response time: #{stats[:avg_response_time]}s"
    else
      puts "No routing statistics available. Enable intelligent routing to track stats."
    end
    
  else
    puts "Unknown routing subcommand: #{subcommand}"
    puts "Available: show, set, reset, stats"
  end
rescue => e
  puts "Routing command error: #{e.message}"
end

def main
  options = {
    model: 'claude-3-5-sonnet-20241022',
    provider: 'anthropic',
    enhanced_ui: false,  # Disabled for now due to display issues
    formatted_output: true,  # Enable formatted responses by default
    url: 'http://localhost:11434',
    region: 'us-east-1',
    secret: 'lantae/api-keys',
    temperature: 0.1,
    auto_accept: false,
    planning_mode: false,
    no_banner: false,
    enable_mcp: false,
    mcp_config: nil,
    agent_mode: false,
    enable_lsp: false
  }

  OptionParser.new do |opts|
    opts.banner = "Usage: #{$0} [options] [prompt]"
    
    opts.on('-m', '--model MODEL', 'Specify the model to use') { |v| options[:model] = v }
    opts.on('-p', '--provider PROVIDER', 'Specify the provider') { |v| options[:provider] = v }
    opts.on('-u', '--url URL', 'Ollama server URL') { |v| options[:url] = v }
    opts.on('-r', '--region REGION', 'AWS region') { |v| options[:region] = v }
    opts.on('-s', '--secret SECRET', 'AWS Secrets Manager secret name') { |v| options[:secret] = v }
    opts.on('-t', '--temperature TEMP', 'Temperature for responses') { |v| options[:temperature] = v.to_f }
    opts.on('-y', '--auto-accept', 'Auto-accept all prompts and confirmations') { options[:auto_accept] = true }
    opts.on('--planning-mode', 'Enable planning mode for complex tasks') { options[:planning_mode] = true }
    opts.on('--agent', 'Enable agent mode with task decomposition') { options[:agent_mode] = true }
    opts.on('--no-banner', 'Disable the startup banner') { options[:no_banner] = true }
    opts.on('--enable-mcp', 'Enable MCP (Model Context Protocol) support') { options[:enable_mcp] = true }
    opts.on('--mcp-config PATH', 'Path to MCP server configuration file') { |v| options[:mcp_config] = v }
    opts.on('--enable-lsp', 'Enable LSP (Language Server Protocol) for code intelligence') { options[:enable_lsp] = true }
    opts.on('--lsp-port PORT', 'LSP server port (default: 7777)') { |v| options[:lsp_port] = v.to_i }
    opts.on('--enhanced-ui', 'Enable enhanced split-screen UI with command queue') { options[:enhanced_ui] = true }
    opts.on('--intelligent-routing', 'Enable intelligent prompt routing through planning/squad/implementation stages') { options[:intelligent_routing] = true }
    opts.on('--routing-config PATH', 'Path to routing configuration YAML file') { |v| options[:routing_config] = v }
    opts.on('-v', '--version', 'Show version') { puts VERSION; exit }
    opts.on('-h', '--help', 'Show this help') { puts opts; exit }
  end.parse!

  if ARGV.empty?
    start_repl(options)
  else
    send_single_prompt(ARGV.join(' '), options)
  end
end

if __FILE__ == $0 || File.basename($0) == 'lantae'
  main
end